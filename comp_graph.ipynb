{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4139, 0.8000, 0.7144, 0.3358, 0.7892, 0.6328, 0.1489, 0.4234, 0.3254,\n",
      "        0.2530])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(10)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6628, 0.4319, 0.2934, 0.6871, 0.8195, 0.5396, 0.0580, 0.9034, 0.8306,\n",
      "        0.5603], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x_with_grad = torch.rand(10, requires_grad=True)\n",
    "print(x_with_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the operations that we can do on both the regular tensor and the tensor with `requires_grad=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2829)\n"
     ]
    }
   ],
   "source": [
    "print((x**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2352, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print((x_with_grad**2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see `grad_fn=<MeanBackwards>` in the output. If any of the tensors in a computation have `required_grad=True`, then the output of any computation will now be a tensor with a `grad_fn` attached to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = (x_with_grad**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MeanBackward0 object at 0x132db49d0>\n",
      "<built-in method name of MeanBackward0 object at 0x132db6170>\n"
     ]
    }
   ],
   "source": [
    "print(res.grad_fn)\n",
    "print(res.grad_fn.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run backpropagation, we populate the `.grad` attribute of our initial tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X prior to backpropagation\n",
      "tensor([0.6628, 0.4319, 0.2934, 0.6871, 0.8195, 0.5396, 0.0580, 0.9034, 0.8306,\n",
      "        0.5603], requires_grad=True)\n",
      "tensor(0.3970, grad_fn=<MeanBackward0>)\n",
      "X after backpropagation: the `grad` is now populated\n",
      "tensor([0.1326, 0.0864, 0.0587, 0.1374, 0.1639, 0.1079, 0.0116, 0.1807, 0.1661,\n",
      "        0.1121])\n"
     ]
    }
   ],
   "source": [
    "print(\"X prior to backpropagation\")\n",
    "print(x_with_grad)\n",
    "res.backward()\n",
    "print(res)\n",
    "print(\"X after backpropagation: the `grad` is now populated\")\n",
    "# print(x_with_grad)\n",
    "print(x_with_grad.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that the gradient calculation is correct. Since it's a mean of ten values, the gradient is just $\\frac{1}{10}$, and since it's a square, the gradient is $*2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1326, 0.0864, 0.0587, 0.1374, 0.1639, 0.1079, 0.0116, 0.1807, 0.1661,\n",
      "        0.1121])\n",
      "tensor([0.1326, 0.0864, 0.0587, 0.1374, 0.1639, 0.1079, 0.0116, 0.1807, 0.1661,\n",
      "        0.1121], grad_fn=<MulBackward0>)\n",
      "tensor([True, True, True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "print(x_with_grad.grad)\n",
    "print(x_with_grad / 10 * 2)\n",
    "print(\n",
    "    torch.isclose(x_with_grad.grad, x_with_grad / 10 * 2, atol=1e-8)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever we call \"backwards\", PyTorch collapses the computational graph and returns the result. In doing so, it frees up memory, which means that we can't call \"backwards\" twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
