{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first define a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "d = 1\n",
    "model = torch.nn.Linear(n, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create fake inputs and look at regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 20\n",
    "\n",
    "x = torch.randn(num_samples, n)\n",
    "y = torch.randn(num_samples, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to now compare our predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6710, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.mse_loss(pred_y, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also write this ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6710, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def mse_loss(pred_y, y):\n",
    "    return torch.mean((pred_y - y) ** 2)\n",
    "\n",
    "loss = mse_loss(pred_y, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For binary classification, we need binary labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "         1., 1.]])\n",
      "tensor([[0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
      "         0., 0.]])\n",
      "tensor(0.5500)\n"
     ]
    }
   ],
   "source": [
    "y = (torch.randn(num_samples, d) > 0).float()\n",
    "print(y.T) # printed as transpose so it prints better on screen.\n",
    "\n",
    "pred_y = (model(x) > 0).float()\n",
    "print(pred_y.T)\n",
    "\n",
    "accuracy = torch.mean((pred_y == y).float())\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function that we use here is binary cross entropy loss, with logits. We want to use \"with logits\" because that loss function will add the log sigmoid for us, meaning that we can just take our regular linear model and directly pass its results to the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(model(x), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7071, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# loss ~ 0.7 means 50/50 chance.\n",
    "# Lower than 0.7 means better than random.\n",
    "# Higher than 0.7 means worse than random.\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multi-class classification, we need to update this since we're looking at the multiple classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 0., 0., 1., 2., 0., 2., 2., 1., 2., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "n_classes = 3\n",
    "model = torch.nn.Linear(n, n_classes)\n",
    "# NOTE: loss function for multi-class classification will\n",
    "# expect integer labels.\n",
    "y = torch.randint(n_classes, (num_samples, 1)).float()\n",
    "print(y.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.functional.cross_entropy(preds, y.squeeze().long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0877, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: don't add the sigmoid or softmax to the \"forward\" method of the class. In the forward method, just do the linear step. Have the loss function take care of that instead.\n",
    "\n",
    "This is for the following reasons:\n",
    "1. Numerical instability: softmax and sigmoid functions can be numerically unstable since you can be dealing with very small numbers. Most loss functions that use these have optimized ways of dealing with this numerical instability, so it's better to have the loss function handle it.\n",
    "2. Log-Sum-Exp Trick: Loss functions like cross-entropy often use the log-sum-exp trick to improve numerical stability. This trick helps in managing the large exponentials that appear in the softmax computation, making the training process more robust.\n",
    "3. More efficient gradients: When the sigmoid or softmax is added in the loss function, PyTorch contains optimizations that make calculating the gradients more efficient.\n",
    "\n",
    "It's better for us to return the raw logits in the forward step and let the optimized loss function implementations handle the numerical instability of the sigmoid and softmax functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
