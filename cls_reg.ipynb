{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "d = 1\n",
    "model = nn.Linear(n, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=10, out_features=1, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1317,  0.0311,  0.2894, -0.0874,  0.0068, -0.2635, -0.2890, -0.2572,\n",
      "         -0.2682, -0.2943]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0242], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.weight)\n",
    "print(model.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0906,  0.3910,  0.0868,  0.0980,  0.0124,  0.3860, -0.6620,  0.4804,\n",
       "         0.0680,  1.3457])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5873], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "\n",
    "Linear binary classification: \n",
    "$$f(X) = \\sigma(Wx+b)$$\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "Maps $$f_\\theta: \\R^{n} \\rightarrow [0,1]$$\n",
    "\n",
    "Examples:\n",
    "- x approaches negative infinity: $x \\rightarrow -\\infty = f(X) \\rightarrow \\frac{1}{\\infty} \\rightarrow 0$\n",
    "- x approaches 0: $x = 0 \\rightarrow f(X) = \\frac{1}{1+e^{0}} \\rightarrow \\frac{1}{1+1} = \\frac{1}{2}$\n",
    "- x approaches infinity: $x \\rightarrow \\infty = f(X) \\rightarrow \\frac{1}{1+0} \\rightarrow 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(torch.nn.Module):\n",
    "    \"\"\"Logistic regression classifier.\"\"\"\n",
    "    def __init__(self, n, d):\n",
    "        super().__init__()\n",
    "        self.model = nn.Linear(n, d)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"We first calculate the linear combination of the features and\n",
    "        weights and then apply a sigmoid function.\"\"\"\n",
    "        regression_output = self.model(x)\n",
    "        # NOTE: do not actually add sigmoid in the \"forward\" function since\n",
    "        # you can't backpropagate without it becoming numerically unstable.\n",
    "        return nn.functional.sigmoid(regression_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc = LinearClassifier(n, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.3162,  0.2263, -0.0120,  0.1590,  0.1616, -0.0768, -0.2428, -0.2046,\n",
       "         -0.1925,  0.2249]], requires_grad=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lc.model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0563, -0.9167, -1.0453,  0.0662,  0.3473,  0.0155,  1.1827, -0.0678,\n",
       "         1.1243, -1.3446])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(n)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3444], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear multi-class classification\n",
    "\n",
    "Multi-class classification uses softmax.\n",
    "$$f_\\theta(x) = \\text{softmax}(Wx+b)$$\n",
    "$$\\text{softmax}(v)_i = \\frac{e^{v_i}}{\\sum_{j=1}^{n} e^{v_j}}$$\n",
    "\n",
    "The probability that a given observation is in the $i^{th}$ class is given by $e^{v_i}$ divided by the probabilities for the observation across all classes, $\\sum_{j=1}^{n} e^{v_j}$.\n",
    "\n",
    "I found this walkthrough quite useful: https://www.pinecone.io/learn/softmax-activation/\n",
    "\n",
    "The softmax does the following to a vector $v=[v_1, ..., v_d]^{T} \\in \\R^{d}$:\n",
    "1. Exponentiates the vector: $e^{v} = [e^{v_1}, e^{v_2}, ..., e^{v_d}]$\n",
    "2. Normalizes that vector by multiplying it by a factor of $$\\frac{1}{\\sum_{i=0}^{d}e^{v_i}}$$\n",
    "\n",
    "Let's work through an example. Let's say that we have $v=[1, -3, 5, -6, 10]$.\n",
    "1. Exponentiate the vector: $e^{v} = [e^1, e^{-3}, e^5, e^{-6}, e^{10}]$.\n",
    "2. Normalize the vector:\n",
    "$$ = [e^1, e^{-3}, e^5, e^{-6}, e^{10}] * \\frac{1}{e^1 + e^{-3} + e^5 + e^{-6} +e^{10}}$$\n",
    "\n",
    "We can calculate this explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1, -3,  5, -6, 10])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([1, -3, 5, -6, 10])\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.7183e+00, 4.9787e-02, 1.4841e+02, 2.4788e-03, 2.2026e+04])\n"
     ]
    }
   ],
   "source": [
    "v_exp = torch.exp(v)\n",
    "print(v_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(22177.6484)\n"
     ]
    }
   ],
   "source": [
    "normalizing_coefficient = torch.sum(v_exp)\n",
    "print(normalizing_coefficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2257e-04, 2.2449e-06, 6.6920e-03, 1.1177e-07, 9.9318e-01])\n"
     ]
    }
   ],
   "source": [
    "normalized_v = v_exp / normalizing_coefficient\n",
    "print(normalized_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see several things here:\n",
    "1. The softmax maintains the orders of its inputs.\n",
    "2. The larger (and more positive) the array's value, the larger the result. The largest number here (the last element) corresponds to the largest value.\n",
    "\n",
    "Given this, we can now take the argmax across this array in order to get our multiclass label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class label: 4\n"
     ]
    }
   ],
   "source": [
    "label = torch.argmax(normalized_v)\n",
    "print(f\"Predicted class label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear multi-class classification example\n",
    "\n",
    "For us to do inference with this, our task has to output multiple values.\n",
    "\n",
    "For example, let's do weather predict:\n",
    "\n",
    "Input: $x$ = day of the way\n",
    "Output: $f(x)$ = precipitation (rain, snow, hail, sun)\n",
    "\n",
    "Prediction:\n",
    "- $\\mathbb{P}$(rain) = $f_0(x)_1$ = $\\text{softmax}(Wx+b)_1$\n",
    "- $\\mathbb{P}$(snow) = $f_0(x)_2$ = $\\text{softmax}(Wx+b)_2$\n",
    "- $\\mathbb{P}$(hail) = $f_0(x)_3$ = $\\text{softmax}(Wx+b)_3$\n",
    "- $\\mathbb{P}$(sun) = $f_0(x)_4$ = $\\text{softmax}(Wx+b)_4$\n",
    "\n",
    "We would have 4 different weight vectors and 4 different bias values.\n",
    "\n",
    "If, in the one-dimensional case, we had $W \\in \\R^{\\text{10 x 1}}$, in this 4-D case we now have $W \\in \\R^{\\text{10 x 4}}$.\n",
    "\n",
    "Our bias, which was $b \\in \\R^{\\text{1x1}}$, now is $b \\in \\R^{\\text{1x4}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize our vectors for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of rain tensor: torch.Size([10])\t Shape of rain bias: torch.Size([1])\n",
      "Rain tensor: tensor([ 0.8005, -0.2246, -1.7709,  0.9279,  0.8335, -1.9097, -0.0217,  0.2062,\n",
      "         0.2626, -0.1394])\t Rain bias: tensor([-0.4068])\n",
      "Shape of snow tensor: torch.Size([10])\t Shape of snow bias: torch.Size([1])\n",
      "Snow tensor: tensor([-0.7112,  0.0871, -0.6663,  0.3715,  0.7649, -1.5910, -0.5670,  0.2699,\n",
      "         0.4071,  0.3503])\t Snow bias: tensor([0.1228])\n",
      "Shape of hail tensor: torch.Size([10])\t Shape of hail bias: torch.Size([1])\n",
      "Hail tensor: tensor([ 1.8832, -0.2612, -0.3553,  0.3730, -1.8256, -0.2604,  1.2536, -0.2062,\n",
      "         0.3728, -2.1392])\t Hail bias: tensor([1.4636])\n",
      "Shape of sun tensor: torch.Size([10])\t Shape of sun bias: torch.Size([1])\n",
      "Sun tensor: tensor([ 0.5999, -0.1561, -0.5780, -1.9524,  0.4827,  0.1182, -0.9592, -1.4935,\n",
      "        -1.2073,  0.9927])\t Sun bias: tensor([0.3712])\n"
     ]
    }
   ],
   "source": [
    "w_rain = torch.randn(n)\n",
    "w_snow = torch.randn(n)\n",
    "w_hail = torch.randn(n)\n",
    "w_sun = torch.randn(n)\n",
    "\n",
    "b_rain = torch.randn(1)\n",
    "b_snow = torch.randn(1)\n",
    "b_hail = torch.randn(1)\n",
    "b_sun = torch.randn(1)\n",
    "\n",
    "print(f\"Shape of rain tensor: {w_rain.shape}\\t Shape of rain bias: {b_rain.shape}\")\n",
    "print(f\"Rain tensor: {w_rain}\\t Rain bias: {b_rain}\")\n",
    "print(f\"Shape of snow tensor: {w_snow.shape}\\t Shape of snow bias: {b_snow.shape}\")\n",
    "print(f\"Snow tensor: {w_snow}\\t Snow bias: {b_snow}\")\n",
    "print(f\"Shape of hail tensor: {w_hail.shape}\\t Shape of hail bias: {b_hail.shape}\")\n",
    "print(f\"Hail tensor: {w_hail}\\t Hail bias: {b_hail}\")\n",
    "print(f\"Shape of sun tensor: {w_sun.shape}\\t Shape of sun bias: {b_sun.shape}\")\n",
    "print(f\"Sun tensor: {w_sun}\\t Sun bias: {b_sun}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now make it 4-D. Each of our weight vectors is a \"row vector\" that we can stack vertically.\n",
    "\n",
    "We need them to be row vectors for our shapes to work out as intended, as our inputs are column vectors by convention.\n",
    "\n",
    "After this, we'll get $W \\in \\R^{\\text{4 x 10}}$ and $b \\in \\R^{\\text{4 x 1}}$\n",
    "\n",
    "We expect our input vectors to be $x \\in \\R^{\\text{10 x 1}}$\n",
    "\n",
    "As a result, $Wx \\in \\R^{\\text{4 x 1}}$ and $y=Wx + b \\in \\R^{\\text{4}}$ We want our bias vector to be a 4-D vector of constants (note, this is different than defining $b \\in \\R^{\\text{4x1}}$ as a matrix. We want it to be a vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of W: torch.Size([4, 10])\t Shape of b: torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "W = torch.stack([w_rain, w_snow, w_hail, w_sun], dim=0)\n",
    "b = torch.stack([b_rain, b_snow, b_hail, b_sun], dim=0).squeeze()\n",
    "\n",
    "print(f\"Shape of W: {W.shape}\\t Shape of b: {b.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's initialize a random vector $x$. Let's say that somehow this represents, say, a vector of the temperatures throughout the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_temps = torch.randn(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now do inference on this vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_temps: torch.Size([10])\n",
      "Shape of W: torch.Size([4, 10])\n",
      "Shape of b: torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of x_temps: {x_temps.shape}\")\n",
    "print(f\"Shape of W: {W.shape}\")\n",
    "print(f\"Shape of b: {b.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.6637, -0.5460,  4.4479, -2.3486])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "y = torch.matmul(W, x_temps) + b\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our result! Now what?\n",
    "\n",
    "We can take the argmax and get the class that we'd want to classify in the first place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class label: 2\n"
     ]
    }
   ],
   "source": [
    "label = torch.argmax(y)\n",
    "print(f\"Predicted class label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we also want to take the softmax as well. Why?\n",
    "\n",
    "1. The raw results can't be interpreted as probabilities, so we can't compare classes against each other. In the context of classification, we want results that will let us compare the probabilities of each class to the other classes.\n",
    "2. Because softmax lets us interpret the results as probabilities, we can use them in loss functions, such as binary cross-entropy, to figure out how well the predicted probabilities match the class labels.\n",
    "3. Backpropagation: softmax is a smooth continuous function, so we can backpropagate.\n",
    "\n",
    "Let's take the softmax of our values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0221, 0.0066, 0.9703, 0.0011])\n"
     ]
    }
   ],
   "source": [
    "softmax_y = torch.nn.functional.softmax(y, dim=0)\n",
    "print(softmax_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the softmax gives us an interpretable set of labels, where we can see each output as the probability that the model assigns the input to the given $i^{th}$ class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's encapsulate this in a torch module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(torch.nn.Module):\n",
    "    \"\"\"Logistic regression classifier.\"\"\"\n",
    "    def __init__(self, n, d):\n",
    "        super().__init__()\n",
    "        self.model = nn.Linear(n, d)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"We first calculate the linear combination of the features and\n",
    "        weights and then apply a sigmoid function.\"\"\"\n",
    "        regression_output = self.model(x)\n",
    "        # NOTE: do not actually add sigmoid in the \"forward\" function since\n",
    "        # you can't backpropagate without it becoming numerically unstable.\n",
    "        return nn.functional.softmax(regression_output, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this again with our previous problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "d = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc = LinearClassifier(n, d)\n",
    "x = torch.randn(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4456, 0.1840, 0.2596, 0.1108], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = lc(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class label: 0\n"
     ]
    }
   ],
   "source": [
    "label = torch.argmax(y)\n",
    "print(f\"Predicted class label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-class vs. multiple binary classification\n",
    "\n",
    "We use the softmax for multiclass classification. But what if we want to predict the probability of multiple classes at the same time? Let's say that instead of predicting the weather as rain, snow, hail, or sun, we say that a day can have any of those labels, even multiple at the same time.\n",
    "\n",
    "In this case, the problem becomes a multiple binary classification problem, since we're predicting the probability that there was rain on a given day, there was snow on a given day, there was hail on a given day, and/or there was sun on a given day.\n",
    "\n",
    "In this case, we would change our function from a softmax to a sigmoid. Using a sigmoid allows for multiple categories and treats them all as independent binary classification problems. The probabilities are uncalibrated (i.e., they don't sum up to one), and so the results are used for multi-labeling tagging.\n",
    "\n",
    "You can have an output like $y=[0.2, 0.6, 0.7, 0.1]$, which clearly doesn't sum up to 1, but indicates that the labels we'd assign to y are $y=[0, 3]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
