{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning in practice\n",
    "\n",
    "What do we do when we actually want to be doing deep learning in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at your data\n",
    "\n",
    "For images, we can do things like:\n",
    "- Sample a random subset of data.\n",
    "- Look at the distribution of labels.\n",
    "- Look at the smallest/largest file size.\n",
    "- Look at common/rare labels. Look for outliers.\n",
    "- Try to solve the task manually and see how you could do it.\n",
    "\n",
    "For other mediums, steps are pretty similar.\n",
    "\n",
    "## Feeding the data into the network\n",
    "\n",
    "### Input normalization\n",
    "For images, you need to resize the outputs as well as normalize the data. If we don't have normalized data, then it's hard to make consistent changes in the network.\n",
    "\n",
    "The solution is mean subtraction. For example, given $x_i$, apply a transformation: $x^*_i = x_i - \\mu_x$. For an image, for example, we'll take the (3 * 64 * 64) input and subtract $(\\mu_\\text{red}, \\mu_\\text{green}, \\mu_\\text{blue})$, or the average red, green, and blue color values across all the images in the dataset.\n",
    "\n",
    "We can also get a case where if the magnitude of the inputs isn't well organized, you can get very uneven training. For example, let's say that you have $|x_1| << |x_2|$. In this case, the gradient caused by $x_2$ will be much larger than the one caused by $x_1$, meaning that the model will attend to only $x_1$.\n",
    "\n",
    "The solution to this then is to divide by the standard deviation as well. For images, this would be the SD of the red, green, and blue color values in the dataset respectively.\n",
    "\n",
    "Therefore, our normalization becomes:\n",
    "$$x^*_i =\\frac{x_i - \\mu_x}{\\sigma_x}$$\n",
    "\n",
    "## Model design\n",
    "\n",
    "We can pick whichever model we want to choose, based on things like complexity and speed. Transformers are pretty popular nowadays though and is probably what should be used in practice.\n",
    "\n",
    "### How do we initialize the network?\n",
    "PyTorch does automatic initialization for us, but it's good to understand what is actually happening in that initialization.\n",
    "\n",
    "#### Can we initialize with zeros?\n",
    "We can't initialize with zeros, since that will lead to a gradient of zeros in the network, meaning that no signal or backpropagation signal will ever be sent. Even if we did get some signal, it would be multiplied by the weights in the backpropagation step, and if the weights are 0, then a signal of 0 will be sent back.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.tensor([1.0, 2.0, 3.0])\n",
    "weights_layer1 = torch.zeros(3, 2, requires_grad=True)\n",
    "weights_layer2 = torch.zeros(2, 1, requires_grad=True)\n",
    "target_tensor = torch.tensor([0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simulate a forward pass and backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_forward_pass(input_tensor):\n",
    "    hidden_layer = torch.matmul(input_tensor, weights_layer1)\n",
    "    hidden_layer_activated = F.relu(hidden_layer)\n",
    "    output_layer = torch.matmul(hidden_layer_activated, weights_layer2)\n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the forward pass\n",
    "output = simple_forward_pass(input_tensor)\n",
    "\n",
    "# Compute loss (MSE)\n",
    "loss = F.mse_loss(output, target_tensor)\n",
    "\n",
    "# Backpropagation\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients for Layer 1 Weights:\n",
      " tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "Gradients for Layer 2 Weights:\n",
      " tensor([[0.],\n",
      "        [0.]])\n",
      "Updated Weights for Layer 1:\n",
      " tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]], requires_grad=True)\n",
      "Updated Weights for Layer 2:\n",
      " tensor([[0.],\n",
      "        [0.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Print gradients to observe the effect of zero initialization\n",
    "print(\"Gradients for Layer 1 Weights:\\n\", weights_layer1.grad)\n",
    "print(\"Gradients for Layer 2 Weights:\\n\", weights_layer2.grad)\n",
    "\n",
    "# Attempt to update weights (hypothetical learning rate of 0.01)\n",
    "with torch.no_grad():\n",
    "    weights_layer1 -= 0.01 * weights_layer1.grad\n",
    "    weights_layer2 -= 0.01 * weights_layer2.grad\n",
    "\n",
    "    # Zero the gradients after updating\n",
    "    weights_layer1.grad.zero_()\n",
    "    weights_layer2.grad.zero_()\n",
    "\n",
    "# Print updated weights to observe the lack of effective update\n",
    "print(\"Updated Weights for Layer 1:\\n\", weights_layer1)\n",
    "print(\"Updated Weights for Layer 2:\\n\", weights_layer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we backpropagate, we see that the gradient is full of zeros. For the last hidden layer, we would get some nonzero signal, since we can, for example, get a loss of $0.5^2$ due to the MSE loss. The problem though is when the loss is multiplied by the weight of 0, in which case the net gradient is 0. When this is backpropagated to the previous layers, we'll get that the gradient in the previous steps is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can we initialize with a constant value?\n",
    "We don't have a way to break symmetries. Let's review this with a motivating example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input tensor\n",
    "input_tensor = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "# 3 input neurons, 2 output neurons, initialized to a constant value of 0.5\n",
    "weights_layer1 = torch.full((3, 2), 0.5)\n",
    "\n",
    "# 2 input neurons, 1 output neuron, initialized to a constant value of 0.5\n",
    "weights_layer2 = torch.full((2, 1), 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do this multiplication, we see that the output of the first layer will be completely symmetrical. This effectively means that the neurons in the first layer are all learning the same thing. Each column in the weight matrix, which is what multiplies against the input vector, is the exact same. The operation that's being done is:\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "1&2&3\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "* \n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "0.5\\\\\n",
    "0.5\\\\\n",
    "0.5\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "= [3]\n",
    "$$\n",
    "\n",
    "$$+$$\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "1&2&3\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "* \n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "0.5\\\\\n",
    "0.5\\\\\n",
    "0.5\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "= [3]\n",
    "$$\n",
    "\n",
    "\n",
    "$$=$$\n",
    "\n",
    "$$[3, 3]$$\n",
    "\n",
    "Each of the 2 weight neurons (here, each weight neuron being a 3x1 tensor) learns the exact same signal. This is no good, since at that point we might as well only have one neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor: tensor([1., 2., 3.])\t Shape: torch.Size([3])\n",
      "Weights of the first layer: tensor([[0.5000, 0.5000],\n",
      "        [0.5000, 0.5000],\n",
      "        [0.5000, 0.5000]])\t Shape: torch.Size([3, 2])\n",
      "Output of the first layer: tensor([3., 3.])\t Shape: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# matrix multiply the input tensor with the weights of the first layer\n",
    "layer1_output = torch.matmul(input_tensor, weights_layer1)\n",
    "print(f\"Input tensor: {input_tensor}\\t Shape: {input_tensor.shape}\")\n",
    "print(f\"Weights of the first layer: {weights_layer1}\\t Shape: {weights_layer1.shape}\")\n",
    "print(f\"Output of the first layer: {layer1_output}\\t Shape: {layer1_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What should we initialize instead?\n",
    "\n",
    "The best initialization that we know is some sort of a random initialization:\n",
    "$$W \\sim \\mathcal{N}(\\mu,\\,\\sigma^{2}I)$$\n",
    "\n",
    "The popular ways to do this are:\n",
    "- Xavier initialization\n",
    "- Kaiming initialization\n",
    "\n",
    "##### Kaiming initialization\n",
    "The Kaiming initialization is a way to initializd the weights of the network layers such that the variance of the outputs from a layer is about equal to the variance of its inputs. The idea here is that this will help maintain a stable gradient flow.\n",
    "\n",
    "We sample some data and then pass them through a randomly initialized network and then tune the weights such that the standard deviation of the activations across the weights is about equal.\n",
    "\n",
    "This is done normally to keep the magnitude of the activations in the network constant, but there is a variant that can be done where we keep the magnitude of the *gradients* constant instead. It's good to do one *or* the other.\n",
    "\n",
    "Kaiming initialization is the default initialization in PyTorch.\n",
    "\n",
    "##### Xavier initialization\n",
    "Xavier initialization is a \"compromise\" version of the Kaiming initialization. Lots of math involved.\n",
    "\n",
    "##### Final layer initialization\n",
    "It is OK to initialize the final layer to zeros, but only if you use SGD. If you use any other optimizer like Adam, using zeros is harmful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000e+00, 0.0000e+00, 1.5274e-43, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        1.5274e-43, 0.0000e+00, 1.4013e-45, 1.4013e-45, 9.2754e-39, 0.0000e+00,\n",
      "        2.4325e+38, 1.4013e-45, 2.4397e+38, 1.4013e-45, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4013e-45, 0.0000e+00,\n",
      "        1.4013e-45, 1.4013e-45, 9.2754e-39, 0.0000e+00, 2.4325e+38, 1.4013e-45,\n",
      "        2.4397e+38, 1.4013e-45, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 1.4013e-45, 0.0000e+00, 1.4013e-45, 1.4013e-45,\n",
      "        9.2754e-39, 0.0000e+00, 2.4325e+38, 1.4013e-45, 2.4397e+38, 1.4013e-45,\n",
      "        0.0000e+00, 0.0000e+00])\n",
      "tensor([-0.2868,  0.3174,  0.1450,  0.1464, -0.1350, -0.1187, -0.2127,  0.2188,\n",
      "         0.0790, -0.0263, -0.2273, -0.1576,  0.1647, -0.2634, -0.3181, -0.1570,\n",
      "        -0.1710, -0.3414,  0.2110, -0.1524,  0.1486, -0.3294,  0.0502, -0.2693,\n",
      "        -0.1220, -0.3193, -0.2725,  0.1503,  0.1354, -0.1946, -0.1188,  0.1695,\n",
      "        -0.2068,  0.0828, -0.3169, -0.0710, -0.0913,  0.1229,  0.1521, -0.1060,\n",
      "        -0.3009,  0.0337, -0.2078,  0.1214,  0.0741, -0.0895,  0.1663, -0.0832,\n",
      "        -0.2632,  0.1365])\n"
     ]
    }
   ],
   "source": [
    "# Example tensor dimensions for a layer with 100 input features and 50 output features\n",
    "kaiming_tensor = torch.empty(100, 50)\n",
    "print(kaiming_tensor[0])\n",
    "torch.nn.init.kaiming_uniform_(kaiming_tensor, mode='fan_in', nonlinearity='relu')\n",
    "print(kaiming_tensor[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principles of model training\n",
    "\n",
    "### Revisiting SGD\n",
    "SGD is a good default optimizer. However, there are pros and cons to using it:\n",
    "- Pros:\n",
    "    - SGD is simple to implement and understand.\n",
    "    - SGD is efficient on large dataset since it takes small batches.\n",
    "    - Great for online learning.\n",
    "    - Converges to the global minimum for convex problems, and can easily escape local minima.\n",
    "- Cons:\n",
    "    - Requires manual tuning of the learning rate.\n",
    "    - The updates are pretty stochastic, which can lead to variance in the optimization path.\n",
    "    - The convergence is slower than other more sophisticated optimizers, especially during later stages of training.\n",
    "\n",
    "These flaws are especially evident later in the training, where we might have wanted to use a large step size or learning rate to learn initial signals, but then this leads to spikes in loss during the later stages as the model can \"overshoot\" when it's coming closer to a desired minima.\n",
    "\n",
    "### Alternatives to SGD\n",
    "\n",
    "#### RMSProp\n",
    "RMSProp attempts to normalize the gradients by their magnitude. This forces the gradient to make more consistent updates throughout the network. RMSProp calculates a running average of the gradient magnitude across batches, and then divides each gradient update by that average (and then applies the momentum term).\n",
    "\n",
    "Frequently updated weights will have their learning rate decreased, since they'll have more larger gradients to average (and therefore, these weights will be decreased more heavily during normalization), while infrequently updated weights will have a higher learning rate. This approach lets us use a relatively large global learning rate and still make stable progress, since the weights are adjusted anyways. It also helps mitigate vanishing and exploding gradients since the gradients are being normalized.\n",
    "\n",
    "#### Adam\n",
    "Adam is similar to RMSProp, but it makes more effective use of the momentum term to make even more sure that the gradients across all the layers are scaled, so that all layers can make almost equal progress. Adam is a pretty popularly used optimizer in practice (and its variant, AdamW, is even more popular). The biggest con with Adam optimizers is the memory requirements to store the gradient normalizations, momentum, and updates, but in practice it works very well.\n",
    "\n",
    "The change in PyTorch is quite simple:\n",
    "\n",
    "```python\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "```\n",
    "\n",
    "### Setting the learning rate\n",
    "How can we determine what learning rate to use?\n",
    "\n",
    "We could try a bunch of learning rates and then see what is the highest that won't give us spikes in loss. However, a more effective way of doing it is setting learning rate schedules.\n",
    "\n",
    "A common way to think about it is to start at a fairly high learning rate, learn until you make progress, and then reduce the learning rate.\n",
    "- Step decay: train until you no longer make progress, then cut the learning rate (by, say, a factor of 2 or a factor of 10), then continue.\n",
    "- Linear decay: decrease the learning rate linearly across training iterations\n",
    "- Cosine decay: popular in industry. Base the learning rate schedule on a cosine decay curve.\n",
    "\n",
    "### Learning rate vs. batch size\n",
    "Often times, bigger batch sizes -> faster convergence. We can take advantages of heuristics for this. For example, if we double the batch size, we can often double the learning rate. This is due in large part to the fact that if you double the batch size, you end up cutting the variance both within a batch and also across batches by a factor of two. It's normally this variance that limits the useful learning rate, so reducing the variance increases the possible learning rate magnitude. This doubling heuristic works decently well for small learning rates, but its usefulness does cap when the learning rate gets large (i.e., starts to approach 1).\n",
    "\n",
    "### Learning rate warmup\n",
    "It can help to gradually increase the learning rate initially and then have it decrease. This is often combined with cosine decay, and the learning rate is spiked up very early in training before it's decreased.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "### Splitting our data\n",
    "We should split our data into training (60%-80%), validation (10%-20%), and a holdout test (10%-20%) set. Ideally we train our model on the training set, validate (and tweak/tune) on the validation set, and then at the very end, after we have a completed and finished model, test just once on the test set. The easiest way to split the data into these subsets is to do random splitting without replacement. But it's important to *check for correlations* in your data (for example, is the distribution of labels different in the training and validation sets?).\n",
    "\n",
    "The validation set checks for overfitting on parameters $\\theta$, since we can see if our model overfit its parameters on the training set. The test set checks for overfitting of hyperparameters since we tune and update those hyperparameters based on what we see in the validation set and it is possible to overfit on the results of the validation set.\n",
    "\n",
    "Overfitting isn't always bad, however, since there's always going to be a degree of overfitting. It becomes bad when our performance on the validation set suffers because we overfit on the characteristics of the training set.\n",
    "\n",
    "### Why do we overfit?\n",
    "\n",
    "#### Sampling bias\n",
    "- The model finds patterns that happen to exist only in the training set. In practice, this happens over the course of epochs, since the model begins to see the same samples multiple times; it's unlikely that the model will overfit on one pass of the data.\n",
    "- Neural networks are really good at creating nonlinear separators of data because it represents the data in a higher dimensional space in which there are cleaner linear separators. The problem is that it's highly unlikely that the representation of two sets of data, such as the training and test set, are identical in a high-enough dimension.\n",
    "\n",
    "### How do we prevent overfitting?\n",
    "\n",
    "#### Early stopping\n",
    "One strategy is to stop training when the validation accuracy peaks. Every few epochs, we can measure the validation accuracy and then save the model, and at the end, we just pick the model that had the highest validation accuracy. We don't necessarily manually stop; rather, we pick the best model after the fact.\n",
    "\n",
    "#### Collect more data\n",
    "We begin to overfit when we sample the same data over and over, so one approach is just collecting more data. This is commonly done these days with LLMs, where there's so much data being used that they don't have to see the same data more than once during training.\n",
    "\n",
    "##### Data augmentation\n",
    "If we can't collect more data, we can do data augmentation. This works especially well for images.\n",
    "\n",
    "##### Transfer learning\n",
    "If we still don't have enough data, we can use transfer learning. We can train a model on a large dataset (pre-training) and then take that model and train it on a target dataset (fine-tuning). The idea is that training on the large dataset learns the \"general distribution\" of a wider task (e.g., for an NLP model, we can use a large dataset to learn general \"language comprehesion\"), and then fine-tune it on a smaller dataset to refine its ability on a narrower task (e.g., comprehension of legal text).\n",
    "\n",
    "In practice, we can download a pretrained model and run a few training iterations on the small dataset.\n",
    "\n",
    "Transfer learning works pretty well because knowledge from one domain or task often transfers well to another and because using a pretrained model means that the weights are already well-initialized.\n",
    "\n",
    "###### When should we use transfer learning?\n",
    "WHENEVER POSSIBLE.\n",
    "\n",
    "For most tasks, similar pretrained models already exists. In early experiments, using a pretrained model gives you a huge head start over creating a model from scratch, and they probably get you most of the way there in whatever task you're working on.\n",
    "\n",
    "### Why do models overfit (deeper dive)\n",
    "The first layer of the model will overfit most closely to the underlying data since it directly touches the data. However, downstream models magnify this overfitting effect because they'll learn to work with the correlations within the upstream overfit layers. This means that often times, deeper layers overfit more since they rely on already overfit activations from previous layers.\n",
    "\n",
    "#### Dropout\n",
    "One way to prevent this is **dropout**, where, with a certain probability, the model will randomly drop the activations and zero them out. This helps with overfitting because downstream layers can't fully rely on the outputs of the upstream layers when making its decisions, so the neurons will be pushed to learn more generalizable patterns. \n",
    "\n",
    "*Note: During testing, we remove this dropout layer. During evaluation, our model needs to be in model.eval model so that the dropout layers switch to being identity matrices.*\n",
    "\n",
    "#### Where to add dropout?\n",
    "In general, we should add dropout:\n",
    "- Before any large fully-connected layer. These have a high number of parameters and connections and they're prone to overfit because they learn to rely on very specific configurations of inputs. Adding dropout to certain activations pushes the weights to learn parameters that are more uncorrelated than the parameters that are learned by other weights.\n",
    "- Before some 1x1 convolutions. These can have a lot of parameters and are used normally for changing the number of channels. Doing it here can be beneficial due to the number of parameters.\n",
    "\n",
    "We should *not* add dropout:\n",
    "- Before general convolutions. Dropout before convolutions can hinder the learning of features by the kernels, plus within the receptive field, there is enough correlation among the different pixels (i.e., even if you dropout a certain pixel, the pixels around it are correlated) that dropout doesn't work well (this is the same principle that makes something like stride work OK since you don't 100% need to analyze every pixel; you can skip around).\n",
    "\n",
    "#### Models can become large\n",
    "If we make our models larger, without any additional modifications, it can (possibly) overfit more. This idea was popularized in classical ML and is less true in deep learning in reality, but we still have to be mindful of it.\n",
    "\n",
    "##### Using smaller models\n",
    "In general, smaller models overfit less, so the principle of using a simpler model is good to be mindful of. However, smaller models also often fit worse and generalize worse, which is something to keep in mind.\n",
    "\n",
    "##### Adding regularization\n",
    "A great way to be able to get the benefits of larger models while minimizing overfitting is throguh using regularization.\n",
    "\n",
    "We can do this by adding weight decay. We can keep the weights small (e.g., L2 regularization) and therefore keep the weight magnitudes small. It also helps with exploding gradients by keeping the magnitudes controlled.\n",
    "\n",
    "In practice, we can just add this to the optimizer (e.g,. AdamW).\n",
    "\n",
    "```python\n",
    "torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "```\n",
    "\n",
    "*In practice this doesn't actually really help overfitting, BUT it is good for making sure that model gradients don't blow up*\n",
    "\n",
    "#### Models can become too complex\n",
    "We can, instead of training a single complex model, try to train multiple small models on the dataset and then average the predictions of the multiple models. This ensemble model works well and the averaging has a good effect. Pre-deep learning, doing ensembling also required using different subsets of data, but in deep learning, there's enough randomness (e.g., by adding randomness to the dataloader or by having random weight initializations).\n",
    "\n",
    "Each model overfits in its own way, so averaging often times leads to the \"true signal\" emerging since a model that overfits in a particular way is averaged out by the other models (and those models' overfitting is averaged out by the others).\n",
    "\n",
    "When to use ensembles?\n",
    "- If you have the compute power - i.e., nearly infinite compute, because it is computationally more expensive\n",
    "- When you need that last 1%-3% of accuracy. Ensembles are popular in competitions like Kaggle.\n",
    "\n",
    "In practice, they're not that popular anymore.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making it work, in practice\n",
    "\n",
    "We can go through an example to see what it would be like to implement these changes in practice.\n",
    "\n",
    "Let's start with a previous ConvNet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(torch.nn.Module):\n",
    "    \"\"\"Convolutional neural network.\"\"\"\n",
    "    class CNNBlock(torch.nn.Module):\n",
    "        def __init__(self, in_channels, out_channels, stride):\n",
    "            super().__init__()\n",
    "            kernel_size = 3\n",
    "            padding = (kernel_size - 1) // 2\n",
    "\n",
    "            self.c1 = torch.nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                padding=padding\n",
    "            )\n",
    "            self.n1 = torch.nn.GroupNorm(1, out_channels)\n",
    "            self.c2 = torch.nn.Conv2d(\n",
    "                out_channels,\n",
    "                out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=1,\n",
    "                padding=padding\n",
    "            )\n",
    "            self.n2 = torch.nn.GroupNorm(1, out_channels)\n",
    "            self.relu1 = torch.nn.ReLU()\n",
    "            self.relu2 = torch.nn.ReLU()\n",
    "\n",
    "            self.skip = torch.nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=1,\n",
    "                stride=stride\n",
    "            ) if in_channels != out_channels else torch.nn.Identity()\n",
    "        \n",
    "        def forward(self, x0):\n",
    "            x = self.relu1(self.n1(self.c1(x0)))\n",
    "            x = self.relu2(self.n2(self.c2(x))) + self.skip(x0)\n",
    "            return x\n",
    "\n",
    "\n",
    "    def __init__(self, channels_l0 = 64, n_blocks = 4):\n",
    "        super().__init__()\n",
    "        kernel_size_l0 = 11\n",
    "        padding_l0 = (kernel_size_l0 - 1) // 2\n",
    "        stride_l0 = 2\n",
    "        # initialize the first layer\n",
    "        cnn_layers = [\n",
    "            torch.nn.Conv2d(\n",
    "                3,\n",
    "                channels_l0,\n",
    "                kernel_size=kernel_size_l0,\n",
    "                stride=stride_l0,\n",
    "                padding=padding_l0\n",
    "            )\n",
    "        ]\n",
    "        c1 = channels_l0\n",
    "        # blocks\n",
    "        for _ in range(n_blocks):\n",
    "            stride = 2\n",
    "            c2 = c1 * stride # to make up for stride\n",
    "            cnn_layers.append(self.CNNBlock(c1, c2, stride))\n",
    "            c1 = c2\n",
    "        \n",
    "        # 1x1 convolution, to output the 102 values that are needed in the\n",
    "        # flowers dataset classification.\n",
    "        cnn_layers.append(torch.nn.Conv2d(c1, 102, kernel_size=1))\n",
    "        self.network = torch.nn.Sequential(*cnn_layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x).mean(dim=-1).mean(dim=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create some training code as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load our data\n",
    "size = (128, 128)\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(size),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "train_dataset = torchvision.datasets.Flowers102(\n",
    "    \"./flowers\", \"train\", transform=transform, download=True\n",
    ")\n",
    "valid_dataset = torchvision.datasets.Flowers102(\n",
    "    \"./flowers\", \"val\", transform=transform, download=True\n",
    ")\n",
    "test_dataset = torchvision.datasets.Flowers102(\n",
    "    \"./flowers\", \"test\", transform=transform, download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ConvNet(channels_l0=32, n_blocks=4)\n",
    "optim = torch.optim.AdamW(net.parameters(), lr=0.005)\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA backend available.\")\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        print(\"Arm mac GPU available, using GPU.\")\n",
    "        device = torch.device(\"mps\") # for Arm Macs\n",
    "    else:\n",
    "        print(\"GPU not available, using CPU\")\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arm mac GPU available, using GPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(11, 11), stride=(2, 2), padding=(5, 5))\n",
       "    (1): CNNBlock(\n",
       "      (c1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (n1): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "      (c2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (n2): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "      (relu): ReLU()\n",
       "      (skip): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2))\n",
       "    )\n",
       "    (2): CNNBlock(\n",
       "      (c1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (n1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "      (c2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (n2): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "      (relu): ReLU()\n",
       "      (skip): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
       "    )\n",
       "    (3): CNNBlock(\n",
       "      (c1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (n1): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
       "      (c2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (n2): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
       "      (relu): ReLU()\n",
       "      (skip): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
       "    )\n",
       "    (4): CNNBlock(\n",
       "      (c1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (n1): GroupNorm(1, 512, eps=1e-05, affine=True)\n",
       "      (c2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (n2): GroupNorm(1, 512, eps=1e-05, affine=True)\n",
       "      (relu): ReLU()\n",
       "      (skip): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
       "    )\n",
       "    (5): Conv2d(512, 102, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_device()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(100):\n",
    "    epoch_loss = []\n",
    "    train_accuracy = []\n",
    "    for data, label in train_loader:\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        output = net(data)\n",
    "        loss = torch.nn.functional.cross_entropy(output, label)\n",
    "\n",
    "        train_accuracy.extend(\n",
    "            (output.argmax(1) == label).cpu().detach().numpy()\n",
    "        )\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        epoch_loss.append(loss.item())\n",
    "    \n",
    "    # note: would normally add validation loop here.\n",
    "    \n",
    "    # early stopping\n",
    "    if epoch % 10 == 0:\n",
    "        # save weights instead of model since saving the\n",
    "        # model also pickles the model architecture,\n",
    "        # which can be OK but is not necessary.\n",
    "        torch.save(net.state_dict(), f\"model_{epoch}.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
