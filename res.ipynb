{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's start with a network that has layer normalization added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModelLN(torch.nn.Module):\n",
    "    def __init__(self, layer_size = [512, 512, 512]):\n",
    "        super.__init__()\n",
    "        layers = []\n",
    "        layers.append(torch.nn.Flatten())\n",
    "        c = 128 * 128 * 3\n",
    "        for s in layer_size:\n",
    "            # bias will be learned by the layer norm\n",
    "            layers.append(torch.nn.LayerNorm(s, bias=False))\n",
    "            layers.append(torch.nn.Linear(c, s))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            c = s\n",
    "        layers.append(torch.nn.Linear(c, 102, bias=False))\n",
    "        self.model = torch.nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual networks are no longer sequential networks. They have \"skip connections\" that skip a bunch of sequential layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefits of residual networks\n",
    "\n",
    "Residual networks reframe the function mapping as `f(x) = x + g(x)`. This re-mapping, where we have to learn the residuals of teh network, has some important benefits when training deep networks. These benefits revolve primarily around being able to overcome the vanishing gradient as well as \"skip\" layers of the network by being able to set residuals to 0 and therefore learn the identity function.\n",
    "\n",
    "Residual networks take us from training 20 layers maximum (with normalization) to >1000 layers.\n",
    "\n",
    "Here's what ChatGPT says about it (good summary actually):\n",
    "\n",
    "### Practical Perspective\n",
    "#### Ease of Training:\n",
    "\n",
    "**Gradient Flow**: Residual connections help in mitigating the vanishing gradient problem by providing a direct path for gradients to flow back through the network during backpropagation. This ensures that even very deep networks can be trained effectively.\n",
    "**Stable Gradients**: By allowing gradients to bypass certain layers, residual connections prevent the gradients from becoming too small (vanishing) or too large (exploding).\n",
    "\n",
    "##### Improved Convergence:\n",
    "\n",
    "**Faster Convergence**: Residual networks often converge faster compared to plain networks because the residual connections facilitate more efficient gradient propagation.\n",
    "**Ease of Optimization**: The skip connections make the optimization landscape smoother, which allows for better and more efficient training.\n",
    "\n",
    "#### Enhanced Feature Propagation:\n",
    "\n",
    "**Direct Information Flow**: Residual connections allow information to flow directly from earlier to later layers, which helps in preserving important features that might otherwise be lost through the depth of the network.\n",
    "**Better Feature Utilization**: Layers can focus on learning residual mappings (the difference between the input and the desired output) rather than the full transformation, making it easier for each layer to learn.\n",
    "\n",
    "### Theoretical Perspective\n",
    "#### Identity Mapping:\n",
    "\n",
    "**Learning Identity Functions**: If the identity mapping (i.e., passing input directly to output) is optimal, residual connections make it easier for the network to learn this mapping. Without residual connections, the network would have to learn this mapping through multiple nonlinear transformations, which is harder.\n",
    "**Expressive Power**: Residual connections increase the expressive power of the network by allowing the network to model both the identity mapping and the residuals. This flexibility makes it easier for the network to learn complex functions.\n",
    "\n",
    "#### Depth and Model Complexity:\n",
    "\n",
    "**Depth with Stability**: Deeper networks can potentially represent more complex functions, but training them is challenging due to issues like vanishing/exploding gradients. Residual connections enable very deep networks to be trained by stabilizing the gradient flow.\n",
    "**Model Regularization**: Residual connections can act as a form of implicit regularization, preventing the model from overfitting by making the network favor learning simpler functions that build upon the identity mapping.\n",
    "\n",
    "#### Mathematical Insights:\n",
    "\n",
    "**Residual Function**: The idea is to reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. Mathematically, for a layer $F(x)$ with input $x$ , the network learns $H(x) = x + F(x)$, where $x$ is the identity mapping.\n",
    "\n",
    "**Ease of Optimization**: Learning residuals is often easier because the network only needs to learn the modifications or differences from the identity function, which is a simpler and more stable task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the residual network\n",
    "\n",
    "Residual networks are no longer sequential, so we need to update our model design accordingly.\n",
    "\n",
    "We'll do this by creating a `Block` class that will include our linear layer and activation function, and then adding the residual connections accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModelLN(torch.nn.Module):\n",
    "    class ResidualBlock(torch.nn.Module):\n",
    "        def __init__(self, in_channels, out_channels):\n",
    "            super().__init__()\n",
    "            self.model = torch.nn.Sequential(\n",
    "                torch.nn.Linear(in_channels, out_channels),\n",
    "                torch.nn.LayerNorm(out_channels),\n",
    "                torch.nn.ReLU()\n",
    "            )\n",
    "\n",
    "            # we need to check if the shapes of the input and output channels\n",
    "            # are the same. If they are, we can directly add +x in the forward\n",
    "            # function. Otherwise, we need to apply a linear transformation to x\n",
    "            # to make the shapes match.\n",
    "            if in_channels != out_channels:\n",
    "                # add a linear layer to the skip connection\n",
    "                self.skip = torch.nn.Linear(in_channels, out_channels)\n",
    "            else:\n",
    "                # do not use a linear layer, but just return the input x\n",
    "                self.skip = torch.nn.Identity()\n",
    "\n",
    "        def forward(self, x):\n",
    "            residual_connection = self.skip(x)\n",
    "            return self.model(x) + residual_connection\n",
    "\n",
    "\n",
    "    def __init__(self, layer_size = [512, 512, 512]):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layers.append(torch.nn.Flatten())\n",
    "        c = 128 * 128 * 3\n",
    "        # normally you don't start with blocks immediately. You normally have\n",
    "        # either a linear layer or a convolutional layer at the beginning.\n",
    "        # For example, in images you'll have a convolutional layer in the\n",
    "        # beginning, whereas in language models you'll have an embedding layer\n",
    "        layers.append(torch.nn.Linear(c, 512, bias=False))\n",
    "        c = 512\n",
    "        for s in layer_size:\n",
    "            layers.append(self.ResidualBlock(c, s))\n",
    "            c = s\n",
    "        layers.append(torch.nn.Linear(c, 102, bias=False))\n",
    "        self.model = torch.nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(10, 3, 128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MyModelLN([512]*4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've generated our network, let's train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
