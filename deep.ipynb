{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building deep networks, step by step\n",
    "\n",
    "Includes steps for building deep networks as well as normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(size),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here, let's download the dataset and keep it in its native format\n",
    "# instead of converting to a list.\n",
    "train_dataset = (\n",
    "    torchvision.datasets.Flowers102(\n",
    "        \"./flowers\", \"train\", transform=transform, download=True\n",
    "    )\n",
    ")\n",
    "test_dataset = (\n",
    "    torchvision.datasets.Flowers102(\n",
    "        \"./flowers\", \"test\", transform=transform, download=True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's look at these datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset Flowers102\n",
       "    Number of datapoints: 1020\n",
       "    Root location: ./flowers\n",
       "    split=train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=(128, 128), interpolation=bilinear, max_size=None, antialias=True)\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are \"container\" objects that implement `__len__()` as well as `__getitem__()`. When we call `__getitem__()` (i.e., if we index it) the image will be loaded from disk and converted to a PyTorch tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a dataloader\n",
    "\n",
    "We now will batch the images into \"minibatches\" using PyTorch's `dataloader` class. That way, we can run our stochastic gradient descent on batches of images instead of individual images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate over the `train_loader` to get each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each batch in `train_loader` contains both the images and the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "2\n",
      "torch.Size([64, 3, 128, 128])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "print(type(single_batch))\n",
    "print(len(single_batch))\n",
    "\n",
    "images, labels = single_batch\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-instantiate the train_loader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we set up our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self, layer_size = [512, 512, 512]) -> None:\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        # for images, we have 3 channels (RGB) and so we need to flatten the\n",
    "        # image to feed it to the first layer. This is done using the Flatten\n",
    "        # layer.\n",
    "        layers.append(torch.nn.Flatten())\n",
    "        c = 128 * 128 * 3\n",
    "        for s in layer_size:\n",
    "            # for each layer, we add a Linear layer followed by a ReLU activation\n",
    "            # We map the initial dimensions (c) to the size of the layer (s)\n",
    "            # and then update c to s for the next iteration.\n",
    "\n",
    "            # for example, if we have layer_size = [512, 512, 512], then the\n",
    "            # layers will be as follows:\n",
    "            # Linear(128*128*3, 512)\n",
    "            # ReLU()\n",
    "            # Linear(512, 512)\n",
    "            # ReLU()\n",
    "            # Linear(512, 512)\n",
    "            # ReLU()\n",
    "\n",
    "            layers.append(torch.nn.Linear(c, s))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            c = s\n",
    "        # finally, we add the output layer which maps the last layer to the\n",
    "        # number of classes in the dataset.\n",
    "        layers.append(torch.nn.Linear(c, 102))\n",
    "\n",
    "        # we use Sequential to stack the layers in order.\n",
    "        self.model = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to set up the other components of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "lr = 0.0001\n",
    "momentum = 0.9\n",
    "epochs = 100\n",
    "optim = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up our training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's see if our logic works for 1 batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs shape: torch.Size([64, 3, 128, 128])\n",
      " labels shape: torch.Size([64])\n",
      " pred shape: torch.Size([64, 102])\n"
     ]
    }
   ],
   "source": [
    "for imgs, labels in train_loader:\n",
    "    pred = model(imgs)\n",
    "    print(f\"imgs shape: {imgs.shape}\\n labels shape: {labels.shape}\\n pred shape: {pred.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can break down the shapes as follows:\n",
    "- We have 64 images, each (3 x 128 x 128)\n",
    "- We have 64 labels\n",
    "- We have 64 preds, each with 102 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the training loop across all minibatches (one epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's set up our training loop. First, let's do just one epoch's worth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.625837802886963\n",
      "Loss: 4.6275835037231445\n",
      "Loss: 4.633312702178955\n",
      "Loss: 4.629975318908691\n",
      "Loss: 4.627366065979004\n",
      "Loss: 4.621416091918945\n",
      "Loss: 4.623439788818359\n",
      "Loss: 4.6265106201171875\n",
      "Loss: 4.621596336364746\n",
      "Loss: 4.6210479736328125\n",
      "Loss: 4.631072044372559\n",
      "Loss: 4.6187543869018555\n",
      "Loss: 4.625478267669678\n",
      "Loss: 4.624572277069092\n",
      "Loss: 4.628574371337891\n",
      "Loss: 4.6273674964904785\n"
     ]
    }
   ],
   "source": [
    "for imgs, labels in train_loader:\n",
    "    pred = model(imgs)\n",
    "    loss_value = loss_fn(pred, labels)\n",
    "    optim.zero_grad()\n",
    "    loss_value.backward()\n",
    "    optim.step()\n",
    "    print(f\"Loss: {loss_value.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss is slowly descending."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the training loop across epochs.\n",
    "\n",
    "Let's now run this for multiple epochs. For this case, let's encapsulate the previous logic in an outer loop across epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 4.603039115667343\n",
      "Epoch 1, Loss: 4.602484464645386\n",
      "Epoch 2, Loss: 4.601881712675095\n",
      "Epoch 3, Loss: 4.6011567413806915\n",
      "Epoch 4, Loss: 4.600630074739456\n",
      "Epoch 5, Loss: 4.599941849708557\n",
      "Epoch 6, Loss: 4.5993668138980865\n",
      "Epoch 7, Loss: 4.598799079656601\n",
      "Epoch 8, Loss: 4.598156690597534\n",
      "Epoch 9, Loss: 4.597478151321411\n",
      "Epoch 10, Loss: 4.596847593784332\n",
      "Epoch 11, Loss: 4.59621199965477\n",
      "Epoch 12, Loss: 4.595578491687775\n",
      "Epoch 13, Loss: 4.594901472330093\n",
      "Epoch 14, Loss: 4.5943571627140045\n",
      "Epoch 15, Loss: 4.593642294406891\n",
      "Epoch 16, Loss: 4.5928977727890015\n",
      "Epoch 17, Loss: 4.592243075370789\n",
      "Epoch 18, Loss: 4.591604918241501\n",
      "Epoch 19, Loss: 4.590942770242691\n",
      "Epoch 20, Loss: 4.5902363657951355\n",
      "Epoch 21, Loss: 4.589542418718338\n",
      "Epoch 22, Loss: 4.588779300451279\n",
      "Epoch 23, Loss: 4.58808708190918\n",
      "Epoch 24, Loss: 4.587442874908447\n",
      "Epoch 25, Loss: 4.586704194545746\n",
      "Epoch 26, Loss: 4.585962802171707\n",
      "Epoch 27, Loss: 4.58517724275589\n",
      "Epoch 28, Loss: 4.584380745887756\n",
      "Epoch 29, Loss: 4.583647161722183\n",
      "Epoch 30, Loss: 4.582870841026306\n",
      "Epoch 31, Loss: 4.582107663154602\n",
      "Epoch 32, Loss: 4.581273794174194\n",
      "Epoch 33, Loss: 4.580536752939224\n",
      "Epoch 34, Loss: 4.579676240682602\n",
      "Epoch 35, Loss: 4.578947186470032\n",
      "Epoch 36, Loss: 4.57809779047966\n",
      "Epoch 37, Loss: 4.577286005020142\n",
      "Epoch 38, Loss: 4.576491951942444\n",
      "Epoch 39, Loss: 4.575612515211105\n",
      "Epoch 40, Loss: 4.574791133403778\n",
      "Epoch 41, Loss: 4.573896765708923\n",
      "Epoch 42, Loss: 4.573020726442337\n",
      "Epoch 43, Loss: 4.572212725877762\n",
      "Epoch 44, Loss: 4.571263462305069\n",
      "Epoch 45, Loss: 4.570418328046799\n",
      "Epoch 46, Loss: 4.56948783993721\n",
      "Epoch 47, Loss: 4.568491756916046\n",
      "Epoch 48, Loss: 4.567669928073883\n",
      "Epoch 49, Loss: 4.566715389490128\n",
      "Epoch 50, Loss: 4.565805673599243\n",
      "Epoch 51, Loss: 4.564829051494598\n",
      "Epoch 52, Loss: 4.563778072595596\n",
      "Epoch 53, Loss: 4.56293112039566\n",
      "Epoch 54, Loss: 4.561897933483124\n",
      "Epoch 55, Loss: 4.560864150524139\n",
      "Epoch 56, Loss: 4.559863269329071\n",
      "Epoch 57, Loss: 4.558909505605698\n",
      "Epoch 58, Loss: 4.557852119207382\n",
      "Epoch 59, Loss: 4.5568326115608215\n",
      "Epoch 60, Loss: 4.555730611085892\n",
      "Epoch 61, Loss: 4.554730445146561\n",
      "Epoch 62, Loss: 4.5536443293094635\n",
      "Epoch 63, Loss: 4.552623152732849\n",
      "Epoch 64, Loss: 4.5514717400074005\n",
      "Epoch 65, Loss: 4.550356537103653\n",
      "Epoch 66, Loss: 4.5491902232170105\n",
      "Epoch 67, Loss: 4.5481646955013275\n",
      "Epoch 68, Loss: 4.546953320503235\n",
      "Epoch 69, Loss: 4.545792102813721\n",
      "Epoch 70, Loss: 4.544521749019623\n",
      "Epoch 71, Loss: 4.5434586107730865\n",
      "Epoch 72, Loss: 4.5422961711883545\n",
      "Epoch 73, Loss: 4.541110336780548\n",
      "Epoch 74, Loss: 4.539730399847031\n",
      "Epoch 75, Loss: 4.538601666688919\n",
      "Epoch 76, Loss: 4.537397682666779\n",
      "Epoch 77, Loss: 4.536002993583679\n",
      "Epoch 78, Loss: 4.534711718559265\n",
      "Epoch 79, Loss: 4.533290892839432\n",
      "Epoch 80, Loss: 4.532091736793518\n",
      "Epoch 81, Loss: 4.530762135982513\n",
      "Epoch 82, Loss: 4.5294189453125\n",
      "Epoch 83, Loss: 4.528136700391769\n",
      "Epoch 84, Loss: 4.52650773525238\n",
      "Epoch 85, Loss: 4.525172650814056\n",
      "Epoch 86, Loss: 4.523973345756531\n",
      "Epoch 87, Loss: 4.522436678409576\n",
      "Epoch 88, Loss: 4.52093118429184\n",
      "Epoch 89, Loss: 4.519444644451141\n",
      "Epoch 90, Loss: 4.5179663598537445\n",
      "Epoch 91, Loss: 4.516421914100647\n",
      "Epoch 92, Loss: 4.514903426170349\n",
      "Epoch 93, Loss: 4.513388514518738\n",
      "Epoch 94, Loss: 4.511610925197601\n",
      "Epoch 95, Loss: 4.51005083322525\n",
      "Epoch 96, Loss: 4.508318990468979\n",
      "Epoch 97, Loss: 4.506616860628128\n",
      "Epoch 98, Loss: 4.505021095275879\n",
      "Epoch 99, Loss: 4.5033445954322815\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    for imgs, labels in train_loader:\n",
    "        pred = model(imgs)\n",
    "        loss_value = loss_fn(pred, labels)\n",
    "        optim.zero_grad()\n",
    "        loss_value.backward()\n",
    "        optim.step()\n",
    "        # print(f\"Loss: {loss_value.item()}\")\n",
    "        losses.append(loss_value.item())\n",
    "    average_loss = sum(losses) / len(losses)\n",
    "    print(f\"Epoch {epoch}, Loss: {average_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally this would run faster if we put the model and the images on GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic form of a network\n",
    "\n",
    "Below is an example of a generic form of a network. We'll use a simple example, independent of the image sample above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(1, 10)\n",
    "        self.fc2 = torch.nn.Linear(10, 1)\n",
    "        self.act = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7369], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(torch.tensor([1.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizations\n",
    "\n",
    "We can now take this and build on it to add normalization.\n",
    "\n",
    "We consider three main types of normalization:\n",
    "- **Layer normalization**: normalizing across all the layers of a single input. For an image, this means collapsing across length, width, and all channels. This means that for a batch of size $n$, there are $n$ means and standard deviations to normalize by.\n",
    "- **Group normalization**: Similar to layer normalization, but you normalize across a group of channels, instead of all channels. Should result in a mean tensor of $n x (c // g)$, where $c$ = # of channels (e.g., 3 in images) and $g$ = group size.\n",
    "- **Batch normalization**: normalizing across each channel in a batch. For color images (which have 3 channels), this would result in a $3x1$ tensor for mean and also for standard deviation, one for each channel.\n",
    "\n",
    "We can add the normalization either before the nonlinearity or after the nonlinearity:\n",
    "- **Before the nonlinearity**: a little tougher since you need to add in a learnable bias and weight after the normalization step. This is because the nonlinearity can remove any signal from the Gaussian output of the normalization centered around 0. This is most evident in a ReLU, where anything less than 0 is filtered out, so therefore any negative values (of which 50% are negative in a Gaussian) will be removed. However, empirically it does have marginally better results than doing it after and so in practice, a lot of people add it before the nonlinearity.\n",
    "- **After the nonlinearity**: easier to implement, since no additional learned weight and bias is needed.\n",
    "\n",
    "In practice, layer normalization is what should be used, unless you're looking at images (in which case, batch normalization can be used).\n",
    "\n",
    "Let's take the same model as before. We have the model below, which has a total of 4 layers (3 hidden, 1 output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self, layer_size = [512, 512, 512]) -> None:\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layers.append(torch.nn.Flatten())\n",
    "        c = 128 * 128 * 3\n",
    "        for s in layer_size:\n",
    "            layers.append(torch.nn.Linear(c, s))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            c = s\n",
    "        layers.append(torch.nn.Linear(c, 102))\n",
    "\n",
    "        # we use Sequential to stack the layers in order.\n",
    "        self.model = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MyModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this for various sizes of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net0(x).norm()=tensor(19.1381, grad_fn=<LinalgVectorNormBackward0>)\n",
      "net1(x).norm()=tensor(7.6016, grad_fn=<LinalgVectorNormBackward0>)\n",
      "net2(x).norm()=tensor(3.2654, grad_fn=<LinalgVectorNormBackward0>)\n",
      "net3(x).norm()=tensor(1.4725, grad_fn=<LinalgVectorNormBackward0>)\n",
      "net4(x).norm()=tensor(1.0034, grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10, 3, 128, 128)\n",
    "net0 = MyModel([])\n",
    "print(f\"{net0(x).norm()=}\")\n",
    "net1 = MyModel([512])\n",
    "print(f\"{net1(x).norm()=}\")\n",
    "net2 = MyModel([512, 512])\n",
    "print(f\"{net2(x).norm()=}\")\n",
    "net3 = MyModel([512, 512, 512])\n",
    "print(f\"{net3(x).norm()=}\")\n",
    "net4 = MyModel([512, 512, 512, 512])\n",
    "print(f\"{net4(x).norm()=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the outputs will get smaller, which means that we're seeing the vanishing gradient happen. Eventually the loss asymptotes, but it's because the bias term keeps some values nonzero. We can check this empirically by setting `torch.nn.Linear(c, s, bias=False)` and `torch.nn.Linear(c, 102, bias=False)` in order to remove the bias terms and then rerunning our experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding batch normalization\n",
    "\n",
    "Let's now update our model definition to add in batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self, layer_size = [512, 512, 512]) -> None:\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layers.append(torch.nn.Flatten())\n",
    "        c = 128 * 128 * 3\n",
    "        for s in layer_size:\n",
    "            layers.append(torch.nn.Linear(c, s))\n",
    "            layers.append(torch.nn.BatchNorm1d(s))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            c = s\n",
    "        layers.append(torch.nn.Linear(c, 102))\n",
    "\n",
    "        # we use Sequential to stack the layers in order.\n",
    "        self.model = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create an arbitrarily large series of networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "netn(x).norm()=tensor(18.3021, grad_fn=<LinalgVectorNormBackward0>)\n",
      "netn(x).norm()=tensor(12.8984, grad_fn=<LinalgVectorNormBackward0>)\n",
      "netn(x).norm()=tensor(13.4140, grad_fn=<LinalgVectorNormBackward0>)\n",
      "netn(x).norm()=tensor(13.6285, grad_fn=<LinalgVectorNormBackward0>)\n",
      "netn(x).norm()=tensor(12.9244, grad_fn=<LinalgVectorNormBackward0>)\n",
      "netn(x).norm()=tensor(13.7696, grad_fn=<LinalgVectorNormBackward0>)\n",
      "netn(x).norm()=tensor(13.3852, grad_fn=<LinalgVectorNormBackward0>)\n",
      "netn(x).norm()=tensor(13.0314, grad_fn=<LinalgVectorNormBackward0>)\n",
      "netn(x).norm()=tensor(13.3189, grad_fn=<LinalgVectorNormBackward0>)\n",
      "netn(x).norm()=tensor(13.1343, grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10, 3, 128, 128)\n",
    "for n in range(10):\n",
    "    netn = MyModel([512] * n)\n",
    "    print(f\"{netn(x).norm()=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the sizes of the outputs remain consistent, due to our normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding layer normalization\n",
    "\n",
    "We can also do this with layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self, layer_size = [512, 512, 512]) -> None:\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layers.append(torch.nn.Flatten())\n",
    "        c = 128 * 128 * 3\n",
    "        for s in layer_size:\n",
    "            layers.append(torch.nn.Linear(c, s))\n",
    "            layers.append(torch.nn.LayerNorm(s))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            c = s\n",
    "        layers.append(torch.nn.Linear(c, 102))\n",
    "\n",
    "        # we use Sequential to stack the layers in order.\n",
    "        self.model = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "netn(x).norm()=tensor(18.6853, grad_fn=<LinalgVectorNormBackward0>)\n",
      "netn(x).norm()=tensor(12.6818, grad_fn=<LinalgVectorNormBackward0>)\n",
      "netn(x).norm()=tensor(12.6936, grad_fn=<LinalgVectorNormBackward0>)\n",
      "netn(x).norm()=tensor(12.8435, grad_fn=<LinalgVectorNormBackward0>)\n",
      "netn(x).norm()=tensor(13.4358, grad_fn=<LinalgVectorNormBackward0>)\n",
      "netn(x).norm()=tensor(12.7709, grad_fn=<LinalgVectorNormBackward0>)\n",
      "netn(x).norm()=tensor(13.6983, grad_fn=<LinalgVectorNormBackward0>)\n",
      "netn(x).norm()=tensor(12.0943, grad_fn=<LinalgVectorNormBackward0>)\n",
      "netn(x).norm()=tensor(12.0792, grad_fn=<LinalgVectorNormBackward0>)\n",
      "netn(x).norm()=tensor(12.8844, grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10, 3, 128, 128)\n",
    "for n in range(10):\n",
    "    netn = MyModel([512] * n)\n",
    "    print(f\"{netn(x).norm()=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a similar effect with layer normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
