{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We do the same setup that we've done before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(size),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = list(\n",
    "    torchvision.datasets.Flowers102(\n",
    "        \"./flowers\", \"train\", transform=transform, download=True\n",
    "    )\n",
    ")\n",
    "test_dataset = list(\n",
    "    torchvision.datasets.Flowers102(\n",
    "        \"./flowers\", \"test\", transform=transform, download=True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = torch.stack([im for im, _ in train_dataset], dim=0)\n",
    "train_labels = torch.tensor([label for _, label in train_dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Linear(128 * 128 * 3, 102)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "lr = 0.01\n",
    "epochs = 10\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only thing that we have to change here is our training loop. Instead of just looping across epochs, we loop across all epochs and per epoch, we loop across each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 8.152144950877561\n",
      "Epoch 1 loss: 8.067449410941057\n",
      "Epoch 2 loss: 6.710525179325545\n",
      "Epoch 3 loss: 7.233729608679183\n",
      "Epoch 4 loss: 6.268898064698453\n",
      "Epoch 5 loss: 5.806133178098848\n",
      "Epoch 6 loss: 5.861039021034764\n",
      "Epoch 7 loss: 6.061175036286352\n",
      "Epoch 8 loss: 5.665907482912174\n",
      "Epoch 9 loss: 5.536450230343481\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    for i in range(0, len(train_images)):\n",
    "        # compute forward pass for one image.\n",
    "        x = train_images[None, i].view(-1, 128 * 128 * 3)\n",
    "        y = train_labels[None, i]\n",
    "        y_pred = model(x)\n",
    "\n",
    "        # compute backward pass\n",
    "        optimizer.zero_grad()\n",
    "        l = loss(y_pred, y)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(l.item())\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    print(f\"Epoch {epoch} loss: {avg_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add batching as well. Batching will help this run faster and should greatly improve convergence as well (since using individual values (i.e., batch_size = 1) can lead to too many update steps happening)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 48.04394211281758\n",
      "Epoch 1 loss: 29.512721039411876\n",
      "Epoch 2 loss: 13.354049857065519\n",
      "Epoch 3 loss: 4.804887838971842\n",
      "Epoch 4 loss: 3.845917576204579\n",
      "Epoch 5 loss: 1.657101103170172\n",
      "Epoch 6 loss: 1.851333588298587e-05\n",
      "Epoch 7 loss: 1.6722442589767184e-05\n",
      "Epoch 8 loss: 1.5262842746111005e-05\n",
      "Epoch 9 loss: 1.404521314363194e-05\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    for i in range(0, len(train_images), batch_size):\n",
    "        # compute forward pass for one image.\n",
    "        x = train_images[None, i].view(-1, 128 * 128 * 3)\n",
    "        y = train_labels[None, i]\n",
    "        y_pred = model(x)\n",
    "\n",
    "        # compute backward pass\n",
    "        optimizer.zero_grad()\n",
    "        l = loss(y_pred, y)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(l.item())\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    print(f\"Epoch {epoch} loss: {avg_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add **momentum** as well as decrease the learning rate. If the loss jumps around, that's normally a sign of the learning rate being too high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Linear(128 * 128 * 3, 102)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "lr = 0.0001\n",
    "momentum = 0.9\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 5.868790969252586\n",
      "Epoch 1 loss: 3.9322223626077175\n",
      "Epoch 2 loss: 1.8542299214750528\n",
      "Epoch 3 loss: 0.8313129461312201\n",
      "Epoch 4 loss: 0.4318122226977721\n",
      "Epoch 5 loss: 0.1006411284324713\n",
      "Epoch 6 loss: 0.05923208067542873\n",
      "Epoch 7 loss: 0.050101972854463384\n",
      "Epoch 8 loss: 0.04454981617163867\n",
      "Epoch 9 loss: 0.040248340694233775\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    for i in range(0, len(train_images), batch_size):\n",
    "        # compute forward pass for one image.\n",
    "        x = train_images[None, i].view(-1, 128 * 128 * 3)\n",
    "        y = train_labels[None, i]\n",
    "        y_pred = model(x)\n",
    "\n",
    "        # compute backward pass\n",
    "        optimizer.zero_grad()\n",
    "        l = loss(y_pred, y)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(l.item())\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    print(f\"Epoch {epoch} loss: {avg_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
