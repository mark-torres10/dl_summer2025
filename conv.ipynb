{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutions\n",
    "\n",
    "This notebook will contain my notes for convolutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Images\n",
    "\n",
    "Let's start with images. Normally they consist of 3 channels (red, green, blue), a height, and a width. Each element of an image will have a pixel value between 0 and 255. Therefore, a given image can have the following dimensions:\n",
    "\n",
    "$\\text{Image} \\in [0, ..., 255]^{C * H * W}$\n",
    "\n",
    "For example, in a 64 x 64 image, you would have (64 * 64) pixels for each of the 3 channels, each taking values between 0 (black) and 255 (\"full color\", either \"pure\" red, green, or blue, depending on the channel).\n",
    "\n",
    "PyTorch does $C * H * W$, or channels first, which was adopted out of a historical design that hasn't been changed. $H * W * C$ ends up actually running faster in practice. This is because in practice, we want to combine operations across channels (e.g., for a given pixel location, collapse all operations across all channels), but when channel ends up first, it leads to the values being located very far apart in memory, versus when channels are last, in which case they're normally closer in memory. This can be fixed with `memory_format = torch.channels_last`, but it's out of the scope of the class plus most of the out-of-the-box PyTorch models and tooling that works with images expects channel format to be included first, so therefore trying to use the $H * W * C$ format can screw up a lot of things.\n",
    "\n",
    "##### Images can be very large\n",
    "If we have, say, a 1024 x 1024 image, this is 1,048,576 pixels per channel, or 3,145,728 pixels across 3 channels. Each pixel value has to be represented by an 8-bit integer since it can take values between 0 and 255.\n",
    "\n",
    "#### Images break fully-connected networks\n",
    "\n",
    "Fully-connected networks do poorly with small transitions and fluctuations in the image.\n",
    "\n",
    "##### Size is a concern\n",
    "For example, let's say that you want to build a cat detector. Let's say that it takes 1024 x 1024 inputs across the 3 channels. In a traditional fully-connected network, we would do the operation $Wx + b$. This operation requires that we represent $x$ as a vector. This is already a concern, because for a 1024 x 1024 image, we would have a vector of length 1,048,576 * 3 = 3,145,728. If we want a weight matrix of size 512, that would mean having a weight matrix of 3,145,728 x 512, or 1,610,612,736 weights. If we use, say, 8-bit weights, that would mean that we'd need 12,884,901,888 bits, corresponding to 1.6GB. For just a single relatively small linear layer, we would need 1.6GB. Therefore, size alone makes this task impractical.\n",
    "\n",
    "In addition, most of the image is actually not that useful for us. For example, do we really need every single pixel from a cat image to detect if something is a cat? A majority of the image is likely just the background. Even the parts of the image that are related, we don't need every single pixel, we just need enough to tell us what we want to know. Therefore, as it turns out, even if we wanted a fully-connected network, most of the weights would turn out to be pretty sparse.\n",
    "\n",
    "In addition, the actual signal is \"diffused\" across the image - there's no one single pixel or point in the image that will tell us that something is a cat, but a \"fuzzy view\" of a region in the image can tell us if something is a cat. Therefore, having weights for every single pixel actually turns out to be counterproductive since having individual weights assumes that the part of the input corresponding to that weight is somehow informative, which in this case, since no single pixel will tell us if something is a cat, makes the weights not very useful (and frankly, likely not much different than noise).\n",
    "\n",
    "##### Image variance\n",
    "\n",
    "Let's say that our model learns how to detect cats. In a fully-connected network, it might learn that images are in the top-left corner of the image (maybe, say, it fires if the values in the square matrix location [15, 15] to [100, 100] are all filled in). What happens if you move the cat to a different part of the picture? Or what happens if you change the size of the cat? Or what happens if you change the shade of the cat (say, it's a white cat)? In those cases, since the weights of a fully-connected layer are matched explicitly to particular indices of the input vector, the fully-connected layer will be susceptible to the slightest changes in the image. The weights that we end up learning are very very specific to particular locations in the vector, which will correspond to particular locations in the image.\n",
    "\n",
    "We need a representation of the image, therefore, that is invariant to changes in location, size, color, and any other ways that we can manipulate the image. Once we have this condensed representation of the image, where we get the same vector regardless of if, for example, a cat is in the top left corner or bottom right corner of an image, we can then pass that vector into the fully connected network.\n",
    "\n",
    "#### Processing images in \"patches\" (i.e., convolutions)\n",
    "\n",
    "##### Benefits of patching\n",
    "We considered the case where we transform the image into a single vector and how that wouldn't work. As an alternative, we can consider processing the image in \"patches\".\n",
    "\n",
    "Let's imagine that we have the following setup:\n",
    "- $\\text{Image} x \\in [0, ..., 255]^{C x H x W}$\n",
    "- Linear layer: $f : \\R^N \\rightarrow \\R^D$\n",
    "\n",
    "We can split the image into $M$ patches, each patch with shape $K * K * 3$ (for 3 color channels), and then run a single linear layer across all the patches (we would need a single linear layer, otherwise we can't communicate information across patches). That gives us the following setup:\n",
    "$$\\R^{CK^2} \\rightarrow \\R^{\\frac{D}{M}}$$\n",
    "This gives us $C * K^2 * \\frac{D}{M}$ parameters total. This is in comparison to $C * H * W * D$ parameters in a regular linear layer. Since each patch takes a square chunk of the image, the number of parameters is approximately equal to:\n",
    "$$C * H * W * \\frac{D}{M} \\approx \\frac{C * H * W * D}{M^2}$$\n",
    "\n",
    "We reduce the number of our parameters by an order of $M^2$.\n",
    "\n",
    "##### Overlapping patching\n",
    "If we cut up our images into patches, we can lose information across patches. We can solve this by slightly overlapping our patches.\n",
    "\n",
    "What we do is we \"slide\" across an image, grab a patch, and take a linear transformation of it.\n",
    "\n",
    "This is called a \"convolution\".\n",
    "\n",
    "![Example of convolution](./../assets/conv_slide_1.png \"Example of convolution\")\n",
    "\n",
    "\n",
    "##### Example convolution\n",
    "\n",
    "Let's go through an example convolution. Let's say that we have the following matrix.\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "1 & 2 & 3\\\\\n",
    "4 & 5 & 6\\\\\n",
    "7 & 8 & 9\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Let's say that we want to take $2 * 2$ patches. This means that we'll take chunks of $2 * 2$ matrices and apply some calculation to get a \"value\" out of the $2 * 2$ matrix. Let's say that we just add the values together. That would mean that we slide the following matrix across each $2*2$ patch:\n",
    "$$\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "1 & 1\\\\\n",
    "1 & 1\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "When we slide this across, we first start with the following patch:\n",
    "$$\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "1 & 2\\\\\n",
    "4 & 5\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Doing our matrix multiplication, we get\n",
    "$$\n",
    "\\left[ \\begin{array}{cc}\n",
    "1 & 2 \\\\\n",
    "4 & 5\n",
    "\\end{array} \\right]\n",
    "\\text{x}\n",
    "\\left[ \\begin{array}{cc}\n",
    "1 & 1 \\\\\n",
    "1 & 1\n",
    "\\end{array} \\right]\n",
    "=\n",
    "12\n",
    "$$\n",
    "\n",
    "Doing this same operation across each $2*2$ window, we get the following matrix result:\n",
    "$$\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "12 & 16\\\\\n",
    "24 & 28\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "This is convolution. We take a linear operation (this case, our sliding windows, which we represented with a $2*2$ matrix) and apply it by sliding across our image. Our sliding windows are called **kernels** and we apply them across the images.\n",
    "\n",
    "We can apply the convolution step mathematically here:\n",
    "\n",
    "![Example of convolution](./../assets/conv_slide_2.png \"Example of convolution\")\n",
    "\n",
    "Let's create a function to demonstrate how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_convolution(input_matrix: torch.Tensor, kernel: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Performs convolution step.\"\"\"\n",
    "    print(f\"Input matrix: {input_matrix}\\t Kernel: {kernel}\")\n",
    "    # Get the dimensions of the input matrix and the kernel\n",
    "    input_matrix_shape = input_matrix.shape\n",
    "    kernel_shape = kernel.shape\n",
    "\n",
    "    # Get the dimensions of the output matrix\n",
    "    output_matrix_shape = (input_matrix_shape[0] - kernel_shape[0] + 1, input_matrix_shape[1] - kernel_shape[1] + 1)\n",
    "\n",
    "    # Initialize the output matrix\n",
    "    output_matrix = torch.zeros(output_matrix_shape)\n",
    "\n",
    "    # Perform the convolution\n",
    "    for i in range(output_matrix_shape[0]):\n",
    "        for j in range(output_matrix_shape[1]):\n",
    "            output_matrix[i, j] = torch.sum(input_matrix[i:i + kernel_shape[0], j:j + kernel_shape[1]] * kernel)\n",
    "\n",
    "    print(f\"Output matrix: {output_matrix}\")\n",
    "    return output_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_matrix = torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "kernel = torch.Tensor([[1, 1], [1, 1]])\n",
    "output = perform_convolution(input_matrix, kernel)\n",
    "expected_output = torch.Tensor([[12, 16], [24, 28]])\n",
    "assert torch.allclose(output, expected_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then have a \"convolution layer\" that takes a series of kernels, slides them across the input matrix, and then returns the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_convolution_layer(input_matrix: torch.Tensor, kernels: list[torch.Tensor]) -> torch.Tensor:\n",
    "    res = [\n",
    "        perform_convolution(input_matrix, kernel)\n",
    "        for kernel in kernels\n",
    "    ]\n",
    "    return torch.stack(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input matrix: tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\t Kernel: tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "Output matrix: tensor([[12., 16.],\n",
      "        [24., 28.]])\n",
      "Input matrix: tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\t Kernel: tensor([[0., 1.],\n",
      "        [0., 1.]])\n",
      "Output matrix: tensor([[ 7.,  9.],\n",
      "        [13., 15.]])\n",
      "Input matrix: tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\t Kernel: tensor([[1., 0.],\n",
      "        [1., 0.]])\n",
      "Output matrix: tensor([[ 5.,  7.],\n",
      "        [11., 13.]])\n",
      "Input matrix: tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\t Kernel: tensor([[1., 0.],\n",
      "        [0., 1.]])\n",
      "Output matrix: tensor([[ 6.,  8.],\n",
      "        [12., 14.]])\n",
      "tensor([[[12., 16.],\n",
      "         [24., 28.]],\n",
      "\n",
      "        [[ 7.,  9.],\n",
      "         [13., 15.]],\n",
      "\n",
      "        [[ 5.,  7.],\n",
      "         [11., 13.]],\n",
      "\n",
      "        [[ 6.,  8.],\n",
      "         [12., 14.]]])\n",
      "torch.Size([4, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "input_matrix = torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "kernels = [\n",
    "    torch.Tensor([[1, 1], [1, 1]]),\n",
    "    torch.Tensor([[0, 1], [0, 1]]),\n",
    "    torch.Tensor([[1, 0], [1, 0]]),\n",
    "    torch.Tensor([[1, 0], [0, 1]])\n",
    "]\n",
    "output = perform_convolution_layer(input_matrix, kernels)\n",
    "print(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This, in essence, is what a convolutional layer does in a neural network. If we have, for example, a $3 * 3$ input and 4 kernels of $2 * 2$, then our output from each kernel will be $2 * 2$, and so across 4 kernels, we'd expect an output of $2 * 2 * 4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Efficiency of the convolution operation\n",
    "\n",
    "For our convolution layer, we save a lot on memory.\n",
    "\n",
    "We don't need individual parameters for a large $(C * H * W) * D$ matrix like we would for a rectangular matrix in a linear layer.\n",
    "\n",
    "Instead, we just need to know what are the parameters in each of the kernels that we slide across the image. We then use this same set of kernels and slide them across all the images that we see. As a result, the # of parameters is independent of image resolution altogether. Whether an image is larger or smaller only affects how many times you \"slide\" across an image; the actual parameters in each of the kernels remains the same.\n",
    "\n",
    "As a result, this becomes very memory efficient.\n",
    "\n",
    "If we take our case as before, with a $1024 * 1024$ image, we would need $5 * 10^{13}$ parameters, but with convolution with $3*3$ kernels, we need <500 parameters.\n",
    "\n",
    "We slide each kernel across the image to get a result, then we add those results together to create a single result for that layer.\n",
    "\n",
    "##### What do we lose with convolution?\n",
    "\n",
    "Convolution greatly compresses our data. But what do we lose in doing so? What we lose is the ability for every input to directly see every output. In a typical $N*D$ rectangular matrix, every output is directly linked to every input somehow. In convolution, however, we aggregate the results of a \"region\" of the input and create the output. We, in essence, get a \"receptive field\" of our results, where a given pixel can only affect how that part of the image is interpreted, not how other parts of the image are interpreted.\n",
    "\n",
    "The deeper that we get in the network, the larger the output's \"receptive field\" is, since it gets more of the aggregated sum of the layers before it. In practice, modern CNNs are deep enough and built in such a way that the final output has a receptive field large enough to encompass the whole image, so it can use information from the entire image. But, in the intermediate layers, these layers have more \"local\" views of the image, where they aggregate local chunks of the image, and downstream layers take these \"aggregated local chunks\" and create \"chunk of chunks\" representations, where they get a view of a larger part of the image by getting fuzzy representations of chunks of the image from the upstream layers. With CNNs, we lose the ability for every layer's outputs to have equal access to all the initial inputs (which we have in FFNNs), but we take a different approach where we ask each layer to create \"chunks\" of representation, and downstream layers take those chunks to create \"chunk of chunks\" representations.\n",
    "\n",
    "##### Convolutions and invariances\n",
    "\n",
    "FFNNs do poorly with image invariances (scaling, moving locations, resizing, etc.). CNNs, because they learn \"chunk of chunks\" representations, do quite well with these image shifts and changes.\n",
    "\n",
    "##### Interpretations of convolution\n",
    "\n",
    "There are generally three ways to interpret the convolution operation:\n",
    "1. Convolution is an **efficient** image processer.\n",
    "2. Convolution preserves **properties** of images.\n",
    "3. Convolution is various **image filters**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building up a convolutional NN\n",
    "\n",
    "What happens if we build our NNs with convolutional layers?\n",
    "\n",
    "Let's say that we have the following normal setup for a NN:\n",
    "\n",
    "$$\\text{Linear} \\rightarrow \\text{ReLU} \\rightarrow \\text{Linear}$$\n",
    "\n",
    "Let's swap out the linear layers for convolutional layers.\n",
    "\n",
    "$$\\text{Convolution} \\rightarrow \\text{ReLU} \\rightarrow \\text{Convolution}$$\n",
    "\n",
    "##### Issue 1: Vanilla convolution shrinks inputs.\n",
    "Because convolutional layers necessarily compress the image into a smaller representation, we end up losing information and making each downstream layer smaller than the previous one.\n",
    "\n",
    "We can solve this with **padding**, where we pad the input with zeros along all sides to match the output size.\n",
    "\n",
    "Padding gives us $p_w$ and $p_h$ (padding width and height) as new hyperparameters. These should always be equal to each other in order to avoid sliding the image. We have to reconsider the shapes of our inputs and outputs:\n",
    "\n",
    "![Convolution with padding](./../assets/conv_slide_3.png \"Convolution with padding\")\n",
    "\n",
    "We should always have $p_w$ such that $w = 2p_w + 1$, or twice the padding plus one equals the kernel width (same for the height). This also means that the kernel sizes should be odd numbers (which they normally are anyways) because having an even-sized kernel shape means that you can't have padding of a correct shape.\n",
    "\n",
    "##### Issue 2: Vanilla CNNs get very slow as $C$, the number of channels, grows\n",
    "\n",
    "**Primer on channels**\n",
    "\n",
    "In each convolutional layer, we apply kernels to extract certain \"features\" of an image. Each kernel gets a result, and then we stack the results of each kernel's sliding into \"channels\", and pass the stack of channels to the next layer.\n",
    "\n",
    "For example, let's say we have $1024 * 1024$ images with 3 channels. Let's also say that we have 32 kernels. Let's say that each kernel preserves size by having appropriate padding. That means that each kernel returns a result of shape $1024 * 1024$. Across 32 kernels, that means a result of shape $1024 * 1024 * 32$.\n",
    "\n",
    "Let's say that the output of the first layer is this output of $1024 * 1024 * 32$. Let's say that in the next layer, we have 16 kernels with shape $3*3$. If we preserve the size of the image, each kernel will have a shape of $3 * 3 * 32$, as it collects $3*3$ chunks across all channels. Each kernel will return a result of shape $1024 * 1024$, meaning that our output from this layer is $1024 * 1024 * 16$.\n",
    "\n",
    "Channels are crucial for CNNs, as they allow the network to learn more rich representations of features as well as supports hierarchical feature learning (i.e., learning \"chunk of chunks\").\n",
    "\n",
    "**Computational cost of convolution**\n",
    "\n",
    "The computational cost of a convolution step can be given by the following:\n",
    "$$O(WHwhC_1C_2)$$\n",
    "\n",
    "Where $W$ and $H$ are the width and height of the input, $w$ and $h$ are the width and height of the kernels, and $C_1$ and $C_2$ are the number of input and output channels respectively.\n",
    "\n",
    "**Problem of too many channels**\n",
    "\n",
    "As our computation continues, we can progressively have more and more channels, which slows down our computation. Since our cost is $O(WHwhC_1C_2)$, if we want to hold the cost constant, we can decrease our image width and height in order to add more kernels (and thus support more channels and therefore more feature extractors).\n",
    "\n",
    "**Solving the problem of slow computation through striding**\n",
    "\n",
    "We can do this through **striding**, where we take a certain step size as we stride across the image. By default, we take a step size of 1 when sliding across the image, but that doesn't have to be the case. We can, for example, take a step size of 2. This has the benefit of reducing the output image size since we're \"skipping\" parts of the image. It also has the benefit of expanding the \"receptive field\" of the downstream layers since the input chunks are no longer overlapping. This is best done by adding striding to later convolutional layers, since those layers want to learn \"higher-order representations\" and have larger receptive fields.\n",
    "\n",
    "![Convolution with stride](./../assets/conv_slide_4.png \"Convolution with stride\")\n",
    "\n",
    "##### What if the CNN is still too slow?\n",
    "\n",
    "If the CNN is still too slow, we can apply two more tricks:\n",
    "\n",
    "**Group convolution**\n",
    "\n",
    "We can split the input channels that we have into groups, and we'll only run certain kernels on certain input channels. That way, our computation is grouped.\n",
    "\n",
    "If we split our input channels into *g* groups, this transforms the computational complexity of our problem.\n",
    "\n",
    "$$O(WHwhC_1C_2) \\rightarrow O(\\frac{WHwhC_1C_2}{g})$$\n",
    "$$O(C_1C_2) \\rightarrow O(\\frac{C_1C_2}{g})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_group_convolution(input_tensor: torch.Tensor, out_channels: int, kernel_size: int, groups: int) -> torch.Tensor:\n",
    "    \"\"\"Performs group convolution.\"\"\"\n",
    "    in_channels = input_tensor.shape[1]\n",
    "    conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size, groups=groups, bias=False)\n",
    "    \n",
    "    # Initialize the kernel weights to some values for demonstration purposes\n",
    "    torch.nn.init.normal_(conv.weight, mean=0.0, std=1.0)\n",
    "    \n",
    "    output = conv(input_tensor)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor Shape: torch.Size([1, 3, 5, 5])\n",
      "Output Tensor Shape: torch.Size([1, 6, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "# Example input tensor with 3 channels, 1 batch, and 5x5 spatial dimensions\n",
    "input_tensor = torch.randn(1, 3, 5, 5)\n",
    "\n",
    "# Perform group convolution with 6 output channels, 2x2 kernel size, and 3 groups\n",
    "output_tensor = perform_group_convolution(input_tensor, out_channels=6, kernel_size=2, groups=3)\n",
    "\n",
    "print(f\"Input Tensor Shape: {input_tensor.shape}\")\n",
    "print(f\"Output Tensor Shape: {output_tensor.shape}\")\n",
    "# print(\"Output Tensor:\", output_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An extreme case of group convolution is **depthwise convolution**, where $C_1 = C_2 = g$, or the input channels and output channels are equal and each channel gets their own group. This reduces the computational cost.\n",
    "\n",
    "$$O(WHwhC_1C_2) \\rightarrow O(WHwhC_1)$$\n",
    "\n",
    "However, this can't apply any processing across channels, so you need to add a $1*1$ convolutional layer right after in order to \"collapse\" across the different channels and process across those channels. The $1*1$ convolution has a runtime of $O(WHC_1C_2)$. Therefore, splitting the convolution operation as a depthwise convolution plus $1*1$ convolution changes our runtime to:\n",
    "\n",
    "$$O(WHwhC_1C_2) \\rightarrow O(WHwhC_1) + O(WHC_1C_2)$$\n",
    "\n",
    "This means transforming our convolution layer from\n",
    "$$\\text{Convolution} \\rightarrow \\text{ReLU}$$\n",
    "\n",
    "To\n",
    "\n",
    "$$\\text{Depthwise convolution} \\rightarrow \\text{1 x 1 convolution} \\rightarrow \\text{ReLU}$$\n",
    "\n",
    "This ends up speeding up the computation of downstream convolution layers and is often applied in deep networks.\n",
    "\n",
    "##### Hyperparameters in convolution\n",
    "\n",
    "We have the following possible hyperparameters in convolution:\n",
    "- Strides\n",
    "- Kernel size\n",
    "- Output channels\n",
    "- Groups\n",
    "- Padding\n",
    "\n",
    "For each of these, there are recommended best practices:\n",
    "- Strides: $s_w = s_h \\in \\{1, 2\\}$\n",
    "- Kernel size: $w = h \\in \\{1, 3\\}$\n",
    "- Output channels: $C_2 = C_1s_w = C_1s_h$ (powers of 2)\n",
    "    - Keeps computation constant: $O(WHwhC_1C_2)$\n",
    "- Groups: ignore, only use if you know what you're doing\n",
    "- Padding: $p_w = \\frac{w-1}{2}, p_h = \\frac{h-1}{2}$\n",
    "\n",
    "#### Building convolutional networks\n",
    "\n",
    "A convolution \"block\" normally consists of\n",
    "- Convolution\n",
    "- Nonlinearity\n",
    "- Normalization and residuals (for deeper networks)\n",
    "\n",
    "We normally have a setup like this:\n",
    "$$\\text{Convolution block} \\rightarrow \\text{Convolution block} \\rightarrow \\text{Linear output layer}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutions in PyTorch\n",
    "\n",
    "Now let's actually implement convolutions in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Conv2d(\n",
    "    in_channels=1, # 1-d input (i.e., not 3-channel input, like RGB).\n",
    "    out_channels=16, # 16 channels output.\n",
    "    kernel_size=3, # 3 x 3 kernels.\n",
    "    stride=1,\n",
    "    padding=1 # (3-1)/2 = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pass in some input. Let's create a random tensor, with batch size of 1, that has only 1 channel. Let's use 28 x 28 since this is the size of the popular MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(\n",
    "    1, # 1 batch.\n",
    "    1, # 1 channel.\n",
    "    28, # 28 x 28 image.\n",
    "    28\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's feed the input through the network and see the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 1, 28, 28])\n",
      "Output shape: torch.Size([1, 16, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input shape: {x.shape}\")\n",
    "y = net(x)\n",
    "print(f\"Output shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution did not change the size of the output (28 x 28) since we added padding. We see that there are now 16 channels in the output instead of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now add striding, we'll see that the output is cut in half:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Conv2d(\n",
    "    in_channels=1, # 1-d input (i.e., not 3-channel input, like RGB).\n",
    "    out_channels=16, # 16 channels output.\n",
    "    kernel_size=3, # 3 x 3 kernels.\n",
    "    stride=2,\n",
    "    padding=1 # (3-1)/2 = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 1, 28, 28])\n",
      "Output shape: torch.Size([1, 16, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input shape: {x.shape}\")\n",
    "y = net(x)\n",
    "print(f\"Output shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build it into a larger network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(torch.nn.Module):\n",
    "    \"\"\"Convolutional neural network.\"\"\"\n",
    "\n",
    "    def __init__(self, layers=[16, 32, 64, 64], kernel_size=3, stride=1):\n",
    "        super(ConvNet, self).__init__()\n",
    "        cnn_layers = []\n",
    "        c1 = 1 # 1 input channel\n",
    "    \n",
    "        # add convolutional layers\n",
    "        for c2 in layers:\n",
    "            padding = (kernel_size - 1) // 2\n",
    "            cnn_layers.append(\n",
    "                torch.nn.Conv2d(\n",
    "                    in_channels=c1,\n",
    "                    out_channels=c2,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=stride,\n",
    "                    padding=padding\n",
    "                )\n",
    "            )\n",
    "            cnn_layers.append(torch.nn.ReLU())\n",
    "            c1 = c2\n",
    "\n",
    "        # add classification layer. We can do this in two ways, either averaging\n",
    "        # the outputs of the last layer using a fully-connected layer or doing\n",
    "        # a 1x1 convolution. Mathematically they are equivalent. Let's do\n",
    "        # the 1x1 convolution, since doing the 1x1 convolution lets us\n",
    "        # maintain the spatial locations of the features to the classification\n",
    "        # layer.\n",
    "        cnn_layers.append(\n",
    "            torch.nn.Conv2d(\n",
    "                in_channels=c1,\n",
    "                out_channels=1, # 1 output class for now.\n",
    "                kernel_size=1\n",
    "            )\n",
    "        )\n",
    "        self.network = torch.nn.Sequential(*cnn_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our classification layer, we could have also used a fully-connected layer instead of the 1x1 convolution. We would modify the last steps of our model definition to be like this:\n",
    "\n",
    "```python\n",
    "        self.conv_layers = torch.nn.Sequential(*cnn_layers)\n",
    "        \n",
    "        # fully connected layer\n",
    "        self.fc = torch.nn.Linear(c1 * 64 * 64, 1)  # Assuming input size is 64x64\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "What's the difference between the two? It lies mostly from the fact that having a linear layer requires that we flatten our convolutional layer. This means that we lose spatial structure in doing so.\n",
    "\n",
    "When we do a 1x1 convolution, we get the following benefits:\n",
    "- **Maintains Spatial Structure**: Since it operates across the depth of the input while maintaining the height and width, it preserves the spatial structure of the input.\n",
    "- **Computational Efficiency**: It performs a linear combination of the input channels at each spatial location, which can be computationally efficient.\n",
    "- **Spatial Features**: It allows the network to maintain and manipulate spatial features up to the final layer. This can be useful for tasks where spatial information is crucial, such as object detection or segmentation.\n",
    "\n",
    "A fully-connected layer, in contrast, requires that we flatten the image. This is more OK in general **classification tasks** and are generally more flexible in terms of input size, especially since they don't care about spatial structure.\n",
    "\n",
    "For our case, we can actually use either output and they'd give similar answers, but it's more computationally efficient to use the 1x1 convolution plus we preserve spatial structure, so we'll use that instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (network): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review what are the output shapes at each step of the network. Let's assume that we're starting with the (28x28) inputs as in our example.\n",
    "\n",
    "Our input is of shape (28 x 28 x 1) since it has one channel.\n",
    "\n",
    "After the first layer, we get $(28 * 28 * 1) \\rightarrow (28 * 28 * 16)$\n",
    "\n",
    "After the second layer, we get $(28 * 28 * 16) \\rightarrow (28 * 28 * 32)$\n",
    "\n",
    "After the second layer, we get $(28 * 28 * 32) \\rightarrow (28 * 28 * 64)$\n",
    "\n",
    "After the second layer, we get $(28 * 28 * 64) \\rightarrow (28 * 28 * 64)$\n",
    "\n",
    "Then, after our 1x1 convolution, we get\n",
    "\n",
    "After the second layer, we get $(28 * 28 * 64) \\rightarrow (28 * 28 * 1)$\n",
    "\n",
    "So the shape of our input is $(28 * 28 * 1)$, and the shape of our output is the same, $(28 * 28 * 1)$.\n",
    "\n",
    "For a classification task, we can then take the 1x1 output and then apply a linear layer, such as:\n",
    "```python\n",
    "torch.nn.Linear(28*28, 1)\n",
    "```\n",
    "\n",
    "But for now, we'll just take an input and return the output CNN representation, to see what we can learn about the receptive field.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 1, 28, 28])\n",
      "Output shape: torch.Size([1, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see the receptive field of a given input. We can do so by passing in a NaN for part of the image and then seeing the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 1, 28, 28)\n",
    "x[0, 0, 10, 10] = float('NaN')\n",
    "y = net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(y[0, 0].detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the input NaN \"infects\" certain areas of the downstream layer. Any element in the output that was affected at all by that one input pixel through the accumulation of different upstream layers is now marked in white, while unaffected outputs maintain their color. This tells us as a result which outputs are affected by the given input.\n",
    "\n",
    "However, this tells us which outputs are affected by which input, and the receptive field would tell us which inputs affect a given output. This is the opposite of what we have (though knowing the above is still interesting), so now we can move on to compute the receptive field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax = plt.subplots(1, 4)\n",
    "\n",
    "x = torch.randn(1, 1, 28, 28, requires_grad=True) # we have to backpropagate\n",
    "\n",
    "# our input image\n",
    "ax[0].imshow(x[0, 0].detach().numpy())\n",
    "y = net(x)\n",
    "\n",
    "# our output image\n",
    "ax[1].imshow(y[0, 0].detach().numpy())\n",
    "\n",
    "# create a new tensor the same shape as y. Doing it with 'new_zeros' copies the type and device location of y to z.\n",
    "# In contrast, torch.zeros creates a tensor on the CPU by default.\n",
    "z = y.new_zeros(y.shape)\n",
    "z[0, 0, z.shape[2]//2, z.shape[3]//2] = float('NaN') # right in the middle of the tensor.\n",
    "\n",
    "# sum all the elements of y*z and backpropagate\n",
    "# given that it's all zeros except for the NaN values, the gradient will be\n",
    "# nonzero only for the NaN values.\n",
    "(y*z).sum().backward()\n",
    "\n",
    "# we can plot our probe z to see where the NaN values are. We should see a\n",
    "# white square in the middle of the image. This corresponds to the output pixel\n",
    "# that we're interested in.\n",
    "ax[2].imshow(z[0, 0].detach().numpy())\n",
    "\n",
    "\n",
    "# now, since we run the backwards step, we can see the gradient of x. Since\n",
    "# there is a NaN that we are backpropagating, we should see a gradient at that\n",
    "# location is NaN and marked with a white square.\n",
    "\n",
    "# as a result, we'll see the inputs in X that are affected by the NaN value\n",
    "# in the output Y. This will tell us which pixels in the input image are\n",
    "# affecting that particular output pixel.\n",
    "ax[3].imshow(x.grad[0, 0].detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling\n",
    "\n",
    "When we apply convolutions to patches of an image, what we're doing is applying a linear operation (represented by our matrix kernels) to the different patches of the image. However, we can generalize this so that instead of applying simply a linear operation, we can apply any function $f$ over our patches. This generalization is known as **pooling**.\n",
    "\n",
    "for pooling, we'll have the following hyperparameters:\n",
    "- Stride\n",
    "- Kernel size\n",
    "- Padding\n",
    "\n",
    "These are the same hyperparameters that we had for our kernels, which makes sense given that pooling is just a generalization of the kernel operation.\n",
    "\n",
    "Pooling is useful for the following reasons:\n",
    "- It reduces the spatial dimensions of the inputs\n",
    "- It reduces overfitting\n",
    "- It provides translation invariance: small shifts in the input image won't significantly affect the results of the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max pooling\n",
    "Max pooling takes the given window and takes the max value in the window. For example:\n",
    "\n",
    "$$\n",
    "\\text{max}\\left(\n",
    "\\begin{matrix}\n",
    "1 & 2\\\\\n",
    "3 & 4\\\\\n",
    "\\end{matrix}\n",
    "\\right) \\rightarrow 4\n",
    "$$\n",
    "\n",
    "This is a nonlinear operation that takes the max value in a given window. It is possible to have a network that is just a set of convolutions and max pooling layers since max pooling is a nonlinear operation. Max pooling is the most commonly used type of pooling. It highlights the most prominent features, such as edges and textures, in the feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 1, 4, 4])\n",
      "Output shape: torch.Size([1, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Example: 2x2 max pooling with stride 2\n",
    "max_pool = torch.nn.MaxPool2d(kernel_size=2, stride=1)\n",
    "\n",
    "# Input tensor\n",
    "input_tensor = torch.tensor([[[[1, 3, 2, 4],\n",
    "                               [5, 6, 1, 2],\n",
    "                               [8, 7, 4, 3],\n",
    "                               [2, 1, 6, 5]]]], dtype=torch.float32)\n",
    "\n",
    "# Apply max pooling\n",
    "output_tensor = max_pool(input_tensor)\n",
    "print(f\"Input shape: {input_tensor.shape}\")\n",
    "print(f\"Output shape: {output_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Average pooling\n",
    "Average pooling takes the average value in a window. For example:\n",
    "$$\n",
    "\\text{average}\\left(\n",
    "\\begin{matrix}\n",
    "1 & 2\\\\\n",
    "3 & 4\\\\\n",
    "\\end{matrix}\n",
    "\\right) \\rightarrow 2.5\n",
    "$$\n",
    "\n",
    "Average pooling is less used than max pooling, and it is generally used when we want a smoothing effect over the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 1, 4, 4])\n",
      "Output shape: torch.Size([1, 1, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Example: 2x2 average pooling with stride 2\n",
    "avg_pool = torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "# Input tensor\n",
    "input_tensor = torch.tensor([[[[1, 3, 2, 4],\n",
    "                               [5, 6, 1, 2],\n",
    "                               [8, 7, 4, 3],\n",
    "                               [2, 1, 6, 5]]]], dtype=torch.float32)\n",
    "\n",
    "# Apply max pooling\n",
    "output_tensor = avg_pool(input_tensor)\n",
    "print(f\"Input shape: {input_tensor.shape}\")\n",
    "print(f\"Output shape: {output_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Related (but not the same): Global pooling\n",
    "Global pooling is a type of pooling that is different from max pooling and average pooling. Global pooling is normally done in the last step of a network and removes the spatial dimension by collapsing a $H * W$ channel to a $1 * 1$ value. Therefore, it collapses a $C * H * W$ input to $C * 1 * 1$ output.\n",
    "\n",
    "There are two types of global pooling:\n",
    "- Global average pooling (GAP): computes the average of the $H*W$ values in a channel.\n",
    "- Global max pooling (GMP): computes the max of the $H*W$ values in a channel.\n",
    "\n",
    "This reduces the number of parameters in the last layer (as opposed to a fully-connected layer), reduces overfitting by reducing model complexity, and helps achieve spatial invariance since the feature map has to be summarized to a spatially-independent single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 1, 4, 4])\n",
      "Output shape: torch.Size([1, 1, 1, 1])\n",
      "Output tensor: tensor([[[[8.]]]])\n"
     ]
    }
   ],
   "source": [
    "# Example: Global max pooling\n",
    "global_max_pool = torch.nn.AdaptiveMaxPool2d((1, 1))\n",
    "\n",
    "# Input tensor\n",
    "input_tensor = torch.tensor([[[[1, 3, 2, 4],\n",
    "                               [5, 6, 1, 2],\n",
    "                               [8, 7, 4, 3],\n",
    "                               [2, 1, 6, 5]]]], dtype=torch.float32)\n",
    "\n",
    "# Apply global max pooling\n",
    "output_tensor = global_max_pool(input_tensor)\n",
    "print(f\"Input shape: {input_tensor.shape}\")\n",
    "print(f\"Output shape: {output_tensor.shape}\") # reduces 4x4 to 1x1\n",
    "print(f\"Output tensor: {output_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 1, 4, 4])\n",
      "Output shape: torch.Size([1, 1, 1, 1])\n",
      "Output tensor: tensor([[[[3.7500]]]])\n"
     ]
    }
   ],
   "source": [
    "# Example: Global average pooling\n",
    "global_avg_pool = torch.nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "# Input tensor\n",
    "input_tensor = torch.tensor([[[[1, 3, 2, 4],\n",
    "                               [5, 6, 1, 2],\n",
    "                               [8, 7, 4, 3],\n",
    "                               [2, 1, 6, 5]]]], dtype=torch.float32)\n",
    "\n",
    "# Apply global average pooling\n",
    "output_tensor = global_avg_pool(input_tensor)\n",
    "print(f\"Input shape: {input_tensor.shape}\")\n",
    "print(f\"Output shape: {output_tensor.shape}\") # reduces 4x4 to 1x1\n",
    "print(f\"Output tensor: {output_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Difference between global pooling and max/average pooling\n",
    "What are the differences between global pooling and max/average pooling? The key difference is when each is done.\n",
    "- **Max/average pooling is done in the intermediate layers**. Its purpose is to reduce the size of the intermediate representations.\n",
    "For example, let's say that we have a $64*64$ image with 3 channels and a $2*2* max pooling operation with stride=1. In that case:\n",
    "    - Our inputs have shape $3 * 64 * 64$\n",
    "    - Our outputs have shape $3 * 63 * 63$ (we apply max pooling across each channel separately and return the reshaped result)\n",
    "- **Global pooling is done in the final layer**. Its purpose is to collapse the results along a spatial dimension.\n",
    "    - Let's say that our input to the last layer has shape $32 * 64 * 64$, for a $64*64$ image across 32 input channels.\n",
    "    - Our output after global pooling will have a shape of $32 * 1 * 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usage of pooling\n",
    "\n",
    "Max and average pooling are no longer commonly used, as other structures have emerged. Max pooling still is sometimes used though as a nonlinearity layer.\n",
    "\n",
    "Before, we would do:\n",
    "$$\\text{Convolution 3x3} \\rightarrow \\text{Max pooling} \\rightarrow \\text{ReLU} \\rightarrow \\text{Convoultion 1x1}$$\n",
    "\n",
    "Now, in lieu of max pooling, we just use striding within the convolutions plus a ReLU nonlinearity, and then add global pooling at the end.\n",
    "\n",
    "$$\\text{Convolution} \\rightarrow \\text{ReLU} \\rightarrow \\text{Convolution} \\rightarrow \\text{ReLU} \\rightarrow \\text{Convolution} \\rightarrow \\text{Global average pooling}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design principles of convolutional networks\n",
    "\n",
    "How can we build bigger and bigger convolutional networks?\n",
    "\n",
    "##### Accounting for striding\n",
    "\n",
    "We want to be able to add striding. This allows us to be able to skip inputs and reduce the dimensionality of our problem. We want to keep the computation constant in the network (so that we have more consistent training throughout the network, the model doesn't overfit to particular input sizes, and the model can be trained and run with more efficient parallel processing), so if we reduce the dimensionality of our inputs, we want a corresponding change to increase the number of channels. If we have a stride of length 2 (which is what is almost always the case), then this will cut our width and height in half, meaning that we should double the number of channels in response.\n",
    "\n",
    "This ends up cutting our number of activations by a factor of 2.\n",
    "\n",
    "$$C * W * H \\rightarrow 2C * \\frac{H}{2} * \\frac{W}{2} \\rightarrow \\frac{C * W * H}{2}$$\n",
    "\n",
    "This ends up not being good across every layer, since we strictly lose information while doing this.\n",
    "\n",
    "Our solution for this is to expand the first layer. In most modern networks, we greatly expand the first layer. The first layer ends up being very very wide, as we represent our inputs with a lot of channels and then condense with striding later on.\n",
    "- Channels: 64 - 96\n",
    "- Large kernel size: 7 - 16: if we blow up a single kernel, we don't get much information, so we need a large kernel size for the first layer.\n",
    "- Strided: 2 - 16\n",
    "\n",
    "##### Keep kernels small\n",
    "Except for the first layer, keep kernels small (use $1*1$ or $3*3$). Larger kernels are often a waste of computation. For example, you can get the effect of a $5*5$ kernel by applying 3 consecutive $3*3$ kernel layers, and those consecutive layers will be more expressive since they'll have nonlinearities (i.e., ReLUs) between them.\n",
    "\n",
    "##### Repeat patterns\n",
    "Create basic blocks for computation, i.e., a \"Convolutional block\". Something like this would suffice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels=1, sizes=[16, 32, 64], kernel_sizes=[3, 1, 3]): # noqa\n",
    "        layers = []\n",
    "        # apply a 3x3 convolution with 16 output channels\n",
    "        # then apply a 3x3 convolution with 32 output channels\n",
    "        # then apply a 3x3 convolution with 64 output channels\n",
    "        # in between, add ReLUs\n",
    "        c1 = in_channels\n",
    "        for c2, kernel_size in zip(sizes, kernel_sizes):\n",
    "            layers.append(\n",
    "                torch.nn.Conv2d(\n",
    "                    in_channels=c1,\n",
    "                    out_channels=c2,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding=kernel_size // 2\n",
    "                )\n",
    "            )\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            c1 = c2\n",
    "        self.network = torch.nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block should then be repeated multiple times, with optional striding within the block. Therefore, your network might look something like this, for a classification case:\n",
    "\n",
    "$$\\text{Input} \\rightarrow \\text{Expanded first layer} \\rightarrow \\text{Conv block} \\rightarrow \\text{Conv block} \\rightarrow \\text{Global average pooling}$$\n",
    "\n",
    "Having a single block makes it easier to develop and tune and debug your network as you're building it.\n",
    "\n",
    "##### Avoid linear layers, keep all layers convolutional\n",
    "Linear layers have too many parameters. Avoid including any linear layers. For the last step, if we were doing classification, we can just use a single $1*1$ convolution (which collapses $C * H * W$ to $1 * H * W$) plus global average pooling (which collapses $1 * H * W$ to a $1*1$ label). Putting the classifier before the global pooling step helps make it more interpretable since we preserve spatial structure up until the very end.\n",
    "\n",
    "We can have the same type of linear operations that we would do in a linear layer and replace them with some combination of convolutions. Using convolutions is orders of magnitude more efficient than linear operations plus learning less weights improves the probability of learning true signal instead of overfitting on noise.\n",
    "\n",
    "We can easily probe the output of the $1*1$ layer, the $1 * H * W$ output, to say, for example, \"does this pixel in the output classify the input image as a dog or as a cat?\", which greatly helps with interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building up a convolutional neural network with PyTorch\n",
    "\n",
    "Let's start with our previous model and build it up using the best practices that we just went over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(torch.nn.Module):\n",
    "    \"\"\"Convolutional neural network.\"\"\"\n",
    "\n",
    "    def __init__(self, layers=[16, 32, 64, 64], kernel_size=3, stride=1):\n",
    "        super(ConvNet, self).__init__()\n",
    "        cnn_layers = []\n",
    "        c1 = 1 # 1 input channel\n",
    "    \n",
    "        # add convolutional layers\n",
    "        for c2 in layers:\n",
    "            padding = (kernel_size - 1) // 2\n",
    "            cnn_layers.append(\n",
    "                torch.nn.Conv2d(\n",
    "                    in_channels=c1,\n",
    "                    out_channels=c2,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=stride,\n",
    "                    padding=padding\n",
    "                )\n",
    "            )\n",
    "            cnn_layers.append(torch.nn.ReLU())\n",
    "            c1 = c2\n",
    "\n",
    "        cnn_layers.append(\n",
    "            torch.nn.Conv2d(\n",
    "                in_channels=c1,\n",
    "                out_channels=1, # 1 output class for now.\n",
    "                kernel_size=1\n",
    "            )\n",
    "        )\n",
    "        self.network = torch.nn.Sequential(*cnn_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up some inputs for the model. Let's use something in the same vein as image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(\n",
    "    1, # batch size of 1\n",
    "    3, # 3 input channels\n",
    "    64, # 64 x 64 image\n",
    "    64\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's update our network accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(torch.nn.Module):\n",
    "    \"\"\"Convolutional neural network.\"\"\"\n",
    "    class CNNBlock(torch.nn.Module):\n",
    "        def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "            super().__init__()\n",
    "            self.conv = torch.nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                padding=kernel_size // 2\n",
    "            )\n",
    "            self.relu = torch.nn.ReLU()\n",
    "        def forward(self, x):\n",
    "            return self.relu(self.conv(x))\n",
    "\n",
    "    def __init__(self, channels_l0 = 64, n_blocks = 4):\n",
    "        super(ConvNet, self).__init__()\n",
    "        cnn_layers = []\n",
    "        kernel_size_l0 = 11 # 11 x 11 kernel for the first layer\n",
    "    \n",
    "        # add first layer + ReLU\n",
    "        cnn_layers.append(\n",
    "            torch.nn.Conv2d(\n",
    "                in_channels=3,\n",
    "                out_channels=channels_l0,\n",
    "                kernel_size=kernel_size_l0,\n",
    "                stride=1,\n",
    "                padding=(kernel_size_l0 - 1)// 2\n",
    "            )\n",
    "        )\n",
    "        cnn_layers.append(torch.nn.ReLU())\n",
    "\n",
    "        # add CNN blocks\n",
    "        c1 = channels_l0\n",
    "        for _ in range(n_blocks):\n",
    "            # for now, let's set the number of output channels to be \n",
    "            # the same as the number of input channels\n",
    "            c2 = c1\n",
    "            cnn_layers.append(self.CNNBlock(c1, c2, kernel_size=3, stride=1))\n",
    "            c1 = c2\n",
    "\n",
    "        # add output 1x1 convolution layer\n",
    "        cnn_layers.append(\n",
    "            torch.nn.Conv2d(\n",
    "                in_channels=c1,\n",
    "                out_channels=1, # 1 output class for now.\n",
    "                kernel_size=1\n",
    "            )\n",
    "        )\n",
    "        # Optional: if we want to compress to a single value (i.e., for\n",
    "        # classification/regression) instead of a 2D map, we can add global\n",
    "        # pooling.\n",
    "\n",
    "        # cnn_layers.append(torch.nn.AdaptiveAvgPool2d(1))\n",
    "\n",
    "        # combine all the layers into a network.\n",
    "        self.network = torch.nn.Sequential(*cnn_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ConvNet(n_blocks=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the network looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: torch.Size([1, 3, 64, 64])\n",
      "Output: torch.Size([1, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 3, 64, 64)\n",
    "y = net(x)\n",
    "print(f\"Input: {x.shape}\")\n",
    "print(f\"Output: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model creates a 2D map out of our image. We take as input a $64 * 64 * 3$ image and output a $64 * 64 * 1$ representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make our network more interesting by adding stride=2 to each block and doubling our number of channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(torch.nn.Module):\n",
    "    \"\"\"Convolutional neural network.\"\"\"\n",
    "    class CNNBlock(torch.nn.Module):\n",
    "        def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "            super().__init__()\n",
    "            self.conv = torch.nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                padding=kernel_size // 2\n",
    "            )\n",
    "            self.relu = torch.nn.ReLU()\n",
    "        def forward(self, x):\n",
    "            return self.relu(self.conv(x))\n",
    "\n",
    "    def __init__(self, channels_l0 = 64, n_blocks = 4):\n",
    "        super(ConvNet, self).__init__()\n",
    "        cnn_layers = []\n",
    "        kernel_size_l0 = 11 # 11 x 11 kernel for the first layer\n",
    "        stride_l0 = 2 # stride of 2 for the first layer\n",
    "    \n",
    "        # add first layer + ReLU\n",
    "        cnn_layers.append(\n",
    "            torch.nn.Conv2d(\n",
    "                in_channels=3,\n",
    "                out_channels=channels_l0,\n",
    "                kernel_size=kernel_size_l0,\n",
    "                stride=stride_l0,\n",
    "                padding=(kernel_size_l0 - 1)// 2\n",
    "            )\n",
    "        )\n",
    "        cnn_layers.append(torch.nn.ReLU())\n",
    "\n",
    "        # add CNN blocks\n",
    "        c1 = channels_l0\n",
    "        for _ in range(n_blocks):\n",
    "            # for now, let's set the number of output channels to be \n",
    "            # the same as the number of input channels\n",
    "            c2 = c1 * 2\n",
    "            stride = 2\n",
    "            cnn_layers.append(self.CNNBlock(c1, c2, kernel_size=3, stride=stride)) # noqa\n",
    "            c1 = c2\n",
    "\n",
    "        # add output 1x1 convolution layer\n",
    "        cnn_layers.append(\n",
    "            torch.nn.Conv2d(\n",
    "                in_channels=c1,\n",
    "                out_channels=1, # 1 output class for now.\n",
    "                kernel_size=1\n",
    "            )\n",
    "        )\n",
    "        # Optional: if we want to compress to a single value (i.e., for\n",
    "        # classification/regression) instead of a 2D map, we can add global\n",
    "        # pooling.\n",
    "\n",
    "        # cnn_layers.append(torch.nn.AdaptiveAvgPool2d(1))\n",
    "\n",
    "        # combine all the layers into a network.\n",
    "        self.network = torch.nn.Sequential(*cnn_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ConvNet(n_blocks=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we print our network, we can see the number of kernels doubling in each block. Our images dimensions are halving but our number of channels is doubling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (network): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(2, 2), padding=(5, 5))\n",
      "    (1): ReLU()\n",
      "    (2): CNNBlock(\n",
      "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (3): CNNBlock(\n",
      "      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (4): CNNBlock(\n",
      "      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (5): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this affects our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: torch.Size([1, 3, 64, 64])\n",
      "Output: torch.Size([1, 1, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 3, 64, 64)\n",
    "y = net(x)\n",
    "print(f\"Input: {x.shape}\")\n",
    "print(f\"Output: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, our inputs were progressively compressed. We reduce the number of channels (from 3 to 1) as well as reduce the shape of our inputs (from 64 * 64 to 4 * 4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now make this more interesting by adding more convolutions to our blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(torch.nn.Module):\n",
    "    \"\"\"Convolutional neural network.\"\"\"\n",
    "    class CNNBlock(torch.nn.Module):\n",
    "        def __init__(self, in_channels, out_channels, stride):\n",
    "            super().__init__()\n",
    "            layers = []\n",
    "            kernel_size = 3\n",
    "            padding = kernel_size // 2\n",
    "\n",
    "            # let's add more convolutions to the block. Let's set the first\n",
    "            # one as having in_channels=in_channels and out_channels=out_channels\n",
    "            # and the next two as having the same input and output shape.\n",
    "            sizes = [out_channels, out_channels, out_channels]\n",
    "            c1 = in_channels\n",
    "            for c2 in sizes:\n",
    "                layers.append(\n",
    "                    torch.nn.Conv2d(\n",
    "                        in_channels=c1,\n",
    "                        out_channels=c2,\n",
    "                        kernel_size=kernel_size,\n",
    "                        padding=padding\n",
    "                    )\n",
    "                )\n",
    "                layers.append(torch.nn.ReLU())\n",
    "                c1 = c2\n",
    "            self.model = torch.nn.Sequential(*layers)\n",
    "        def forward(self, x):\n",
    "            return self.model(x)\n",
    "\n",
    "    def __init__(self, channels_l0 = 64, n_blocks = 4):\n",
    "        super(ConvNet, self).__init__()\n",
    "        cnn_layers = []\n",
    "        kernel_size_l0 = 11 # 11 x 11 kernel for the first layer\n",
    "        stride_l0 = 2 # stride of 2 for the first layer\n",
    "    \n",
    "        # add first layer + ReLU\n",
    "        cnn_layers.append(\n",
    "            torch.nn.Conv2d(\n",
    "                in_channels=3,\n",
    "                out_channels=channels_l0,\n",
    "                kernel_size=kernel_size_l0,\n",
    "                stride=stride_l0,\n",
    "                padding=(kernel_size_l0 - 1)// 2\n",
    "            )\n",
    "        )\n",
    "        cnn_layers.append(torch.nn.ReLU())\n",
    "\n",
    "        # add CNN blocks\n",
    "        c1 = channels_l0\n",
    "        for _ in range(n_blocks):\n",
    "            # for now, let's set the number of output channels to be \n",
    "            # the same as the number of input channels\n",
    "            c2 = c1 * 2\n",
    "            stride = 2\n",
    "            cnn_layers.append(self.CNNBlock(c1, c2, stride=stride)) # noqa\n",
    "            c1 = c2\n",
    "\n",
    "        # add output 1x1 convolution layer\n",
    "        cnn_layers.append(\n",
    "            torch.nn.Conv2d(\n",
    "                in_channels=c1,\n",
    "                out_channels=1, # 1 output class for now.\n",
    "                kernel_size=1\n",
    "            )\n",
    "        )\n",
    "        # Optional: if we want to compress to a single value (i.e., for\n",
    "        # classification/regression) instead of a 2D map, we can add global\n",
    "        # pooling.\n",
    "\n",
    "        # cnn_layers.append(torch.nn.AdaptiveAvgPool2d(1))\n",
    "\n",
    "        # combine all the layers into a network.\n",
    "        self.network = torch.nn.Sequential(*cnn_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how our network looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ConvNet(n_blocks=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (network): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(2, 2), padding=(5, 5))\n",
      "    (1): ReLU()\n",
      "    (2): CNNBlock(\n",
      "      (model): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (3): ReLU()\n",
      "        (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (5): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (3): CNNBlock(\n",
      "      (model): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (3): ReLU()\n",
      "        (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (5): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (4): CNNBlock(\n",
      "      (model): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (3): ReLU()\n",
      "        (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (5): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (5): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see that our blocks have multiple convolutional layers within them. We can also start to add normalizations and residuals within these blocks, like we did for the fully connected network.\n",
    "\n",
    "The basic structure of a CNN is the following:\n",
    "\n",
    "$$\\text{Input} \\rightarrow \\text{Conv} \\rightarrow \\text{ReLU} \\rightarrow \\text{Conv Blocks} \\rightarrow \\text{1x1 Conv} \\rightarrow \\text{Optional (i.e., for classification) Global average pooling}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dilation and upconvolution\n",
    "\n",
    "##### Recap: receptive fields with large kernels\n",
    "\n",
    "Receptive fields only grow slowly across layers. As the slide shows below, for us to increase our receptive field we need to either increase the kernel size or increase the number of layers, both of whcih are computationally expensive.\n",
    "\n",
    "At each layer, our receptive field grows by +2 (+1 on each side).\n",
    "\n",
    "![Receptive fields with large kernels](./../assets/conv_slide_5.png \"Receptive fields with large kernels\")\n",
    "\n",
    "##### Receptive fields with strides\n",
    "Since striding allows us to skip inputs, our receptive field begins to grow much quicker. The growth in the receptive fields begins to exponentially increase, until we quickly get layers whose receptive fields are larger than the whole image.\n",
    "\n",
    "![Receptive fields with striding](./../assets/conv_slide_6.png \"Receptive fields with striding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
